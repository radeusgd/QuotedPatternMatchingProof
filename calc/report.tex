% Template by:
% for 18/19 Trimester 2
% Chia Jason 1161300548
% Hor Sui Lyn 1161300122
%
\documentclass[runningheads]{article}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage[fleqn]{amsmath}
\usepackage{stmaryrd}
\usepackage{listings}
%\usepackage{minted}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{pxfonts}
\usepackage{todonotes}
\usepackage{wasysym} % lambda circle symbol
%\usepackage{languages}
\usepackage{bcprules}     % typeset inference rule
\usepackage{multicol}     % multi-column

\usepackage{framed}

%\newtheorem*{theorem-no-num}{Theorem}

\setlength{\marginparwidth}{2cm} %% TODO remove - used for TODO
\newcommand{\calculus}{$\lambda^{\RIGHTcircle}$} 
\lstset{basicstyle=\ttfamily}
%\lstdefinestyle{pdot}{
%  %numbers=left,
%  %stepnumber=1,
%  %numbersep=10pt,
%  %tabsize=4,
%  showspaces=false,
%  showstringspaces=false,
%  mathescape=true,
%  breaklines=true,
%  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},linewidth=1.1\textwidth,
%  %xleftmargin=-0.1\textwidth
%}
%\lstset{basicstyle=\small\ttfamily}

% \newcommand{\fresh}[1]{\boxed{#1}}

\begin{document}
% title
\title{Mechanized proof for Quoted Pattern Matching in Scala 3}
%\thanks{Supervised by} % if we want to thank someone/org.
%
% authors
\author{Radosław Waśko}
%\author{Chia Jason\inst{1}\orcidID{0000-0002-2056-1687} \and
%Hor Sui Lyn\inst{1}\orcidID{0000-0003-2614-374X}}

%
%\authorrunning{R. Waśko}
% First names are abbreviated in the running head.
%
%\institute{{EPFL}}
%
\maketitle              % typeset the header of the contribution
%formal
\begin{abstract}
  %%% https://plg.uwaterloo.capaper/~migod/research/beckOOPSLA.html
The pattern matching on code from the new macro system of Scala 3 is modelled by a calculus with staging. We present a mechanized proof in Coq of soundness of that calculus.
%
%\keywords{Attendance System  \and Parameter tampering \and Exploit discovery.}
\end{abstract}
%
%
\section{Introduction}

Scala 3 is getting a new macro system based on staged computation \cite{dottydocMacro} which also supports pattern matching over code values. The theory behind this addition is described in \textit{Quoted Pattern Matching in Scala 3} paper \cite{QPM}. 

For example, one can quote an expression and pattern match over it to transform it \footnote{Example from \cite{QPM}}
\begin{verbatim}
val code = '{ println(1+2) }
code match
case '{ println($x)) } =>
  '{ println("Result of " + ${Expr(x.show)} + ": " + $x) }
  // the generated code will print: "Result of 1+2: 3"
\end{verbatim}

The paper introduces $\lambda^{\RIGHTcircle}$ calculus that extends the STLC with quotes, splices and pattern matching over quoted code values. The calculus is explained in more detail in section \ref{sec:calculus}.

The goal of this semester project was to create mechanized proofs of soundness of that calculus, based on the paper proofs in the original paper. The project consists of 1366, of which 585 are the proofs and 455 are definitions. The definitions are explained in section \ref{sec:definitions} and the proofs in section \ref{sec:proving}.

While working on the proofs we had to solve some problems that may be useful in other works. We discuss these in section \ref{sec:discussion}.

\section{$\lambda^{\RIGHTcircle}$ calculus}
\label{sec:calculus}

The system is formalized in two parts - there is an \textit{implicitly-typed} language that is then elaborated to an \textit{explicitly-typed} variant in which every term is ascribed with its type. The ascribed types are used as evidence in the operational semantics.

The type system has three forms: a base type for natural numbers ($Nat$), a function type ($T_1 \to T_2$) and a type for code values ($\square T$).

The syntax contains the usual constructs from STLC: lambda abstraction, application and variables. Moreover it has a natural number constants and a fixpoint operator allowing recursion.

It is then extended with quote ($\square e$) and splice ($\$e$) operators typical for staged computation, a \verb|lift| operator allowing to lift a numeric value to a code value returning that number and a simple pattern match construct ($e \sim p \; ? \; e_{succ} \; || \; e_{fail}$).
The pattern match checks if $e$ matches the pattern $p$ and if it does, reduces to $e_{succ}$ where any bindings introduced by the pattern are available, otherwise it reduces to $e_{fail}$.

A non-reducible value in this calculus can be a natural number, an unapplied lambda or a quoted plain code value. 

A plain term can only contain the non-staged subsed of the calculus - lambdas, abstractions, constants and the fixpoint. Quotes, splices and pattern matching are excluded in plain terms.

Nesting of quotes is spliced is expressed by term's level - level 0 terms are the base terms that are executed and level 1 terms are the boxed code values (which can also contain level 0 terms by splicing). For simplicity, the calculus only supports the two levels.

Below is an example of a term in the calculus, implementing a function that matches application of a function taking a number and replaces its argument with $42$, if the match fails the original code is returned. (We skip the explicit types for clarity):

\begin{equation*}
  \Big(\lambda e: \square Nat. \; e \sim \big((\texttt{bind} [Nat \to Nat] \; f) \; (\texttt{bind} [Nat] \; x)\big) \; ? \; \square(\$f \; 42) \; || \; e \Big) \;\; \square((\lambda x: Nat. \; x) \; 10)
\end{equation*}

This term will reduce to a code value with the same function but applied to the different argument: $\square((\lambda x: Nat. \; x) \; 42)$.

\section{Definitions in Coq}
\label{sec:definitions}
We define the \textit{explicitly-typed} variant of the calculus, as the safety properties are defined for it and the elaboration rules are quite simple.

Defining the calculus in Coq required some simplifications to be able to express it in a reasonable way. We tried to only allow simplifications that don't limit the expressivity of the language - they are mostly changing the syntax and simplifying some constructs. Most of the simplifications could be handled with some form of syntactic rewriting.

\subsection{Defining variable binding}

Variable binding is a common issue that is encountered in all projects that try mechanizing proofs about calculi. 

On paper we just use textual names and define a capture-avoiding substitution operation. This is possible in Coq, but this kind of function is not so easy to define - we need to apply recursively on terms that are not structurally sub-terms of the main terms (because sometimes they may be renamed etc.). As Coq only accepts total functions, we need to show that the invocations are indeed on tems smaller in some way. This however highly complicates the definitions and also makes it harder to reason about theorems using such a complex function.

\subsubsection{De Bruijn indices}

The usual solution to this problem is using De Bruijn indices \cite{Bruijn1972LambdaCN}. In a term encoded using De Bruijn indices, variable names are replaced by indices - numbers that indicate how many binders are in scope (in the expression tree) between the variable and the binder that this variable is referring to. For example the variable referring to the closest lambda gets index 0 (or depending on convention, it can start with 1, but we will start with 0), one referring to the closest lambda after that one gets 1 etc. In such notation, $\alpha$-equivalence is reduced to syntactic equality and defining capture avoiding substitution is much simpler.

For example, the identity function $\lambda x. \, x$ is encoded as $\lambda. \, \#0$ and the K combinator $\lambda x. \, \lambda y. \, x$ becomes $\lambda. \, \lambda. \, \#1$.

All the free variables also have to be remapped to natural numbers. An index greater than the number of binders that it is surrounded by refers to free variables. For example the terms $\#0$ and $\lambda. \#1$ refer to the first free variable, $\#1$ to the second one etc.

When substituting, it is important to keep track of the index - when traversing inside of a binder it has to be updated to still refer to the same variable.
For example, when reducing $(\lambda. \, \lambda. \, \#1) \; (\lambda. \, \#2)$, $\#2$ refers to the second free variable (we have to skip one binder, so we refer to the free variable number $1$, but as we are $0$-indexing, that is the second free variable), after $\beta$-reduction, we get $\lambda. \lambda. \, \#3$ - the substituted term had to be shifted to still refer to the second free variable (as it is now behind not one but two binders).

\subsubsection{DbLib}

As handling variable binding is common to nearly all calculi formalizations, there are some Coq libraries that help automating parts of reasoning about them. In this project we are using the DbLib library \cite{dblib}, the reasons for choosing it are discussed in section \ref{binderlibs}.
\label{traverse}
When using DbLib we freely define our syntactic forms as some inductive type. We have two types - typed terms (\texttt{typedterm}) and untyped terms (\texttt{term}), and we can substitute an untyped term into both of these types. To use the automation facilities provided by the library, we need to define:
\begin{itemize}
  \item an instance of \texttt{Var} typeclass which is used for constructing variables - it requires a function \texttt{var} that takes a De Bruijn index (which is simply a natural number) and returns an instance of the type that we will be substituting (in our case \texttt{term}); for us the instance is just calling the constructor \texttt{var x = Var x}
  \item instances of \texttt{Traverse} typeclass which requires a function \texttt{traverse} taking 3 parameters: a function \texttt{f} representing the operation that is applied to the term-tree, a natural number \texttt{l} representing the current level and a term \texttt{t} that is traversed. It has to recursively traverse all subterms in \texttt{t}, increasing \texttt{l} when entering a binder, and each encountered instance of a variable \texttt{Var x} has to be replaced with \texttt{f l x}.
\end{itemize}
Afterwards some lemmas have to be proven to show that the \texttt{traverse} function satisfied the required constraints - for example it has to be injective. If the function is defined reasonably, these properties are derived automatically by tactics provided by the library.

Once this is defined, substitution, shift etc. are automatically derived by using the \texttt{traverse} function.

\subsection{Defining patterns}

Defining patterns is particularly complicated as the patterns can be arbitrarily nested, so a pattern match could introduce an arbitrary amount of bindings. This is possible to handle, but is much harder to reason about. To simplify the reasoning, we forbid nested patterns. This does not limit the expressivity of the calculus as any nested pattern match can be converted to a series of non-nested patterns.

For example, we can convert 
\[
e \sim (\texttt{lam}[Nat \to Nat] \, f) \; 42 \; ? \; f \; \square(10) \; || \; e_{else}
\] into \[
e \sim (\texttt{MatchApp} \, [Nat \to Nat] \, a_1 \, [Nat] \, a_2) \; ? \; \Bigg(a_1 \sim (\texttt{MatchLam}[Nat \to Nat] \, f) \; ? \; \big(a_2 \sim \texttt{MatchNat} \, 42 \; ? \; (f \; \square(10))\; || \; e_{fail}\big)\; || \; e_{fail}\Bigg)\; || \; e_{fail}
\]
(where \texttt{MatchApp} pattern is a special pattern which matches an application and binds the first and second term to $a_1$ and $a_2$ respectively).

After simplifying to non-nested patterns, the \texttt{bind} pattern is not useful anymore, so we remove it (in non-nested approach it would be equivalent to a let statement). However all of the other patterns that can bind something are changed in a way that they emulate containing the BIND, as shown in the example above. Ultimately, we have the following patterns: Nat, Var, Fix, App, Unlift, Abs, Lam.

Even after the simplification, the App pattern still introduces two variable bindings, so we need to be able to support it. This is still much simpler than the arbitrary amount of bindigs that we started with.

We first tried to define the patterns as a separate inductive definition and have a single syntactic form for the pattern matching:
\begin{verbatim}
Inductive pattrn :=
| PNat (n : nat)
| PVar (x : DeBruijnIndex)
| PBindUnlift
| PBindApp (T1 T2 : type)
| PBindLam (T : type).

Inductive typedterm :=
| TypedTerm (u: term) (t: type)
with term :=
| Nat (n : nat)
| Var (x : DeBruijnIndex)
| Lam (argT: type) (ebody : typedterm)
| App (e1 e2 : typedterm)
| Fix (e : typedterm)
| Lift (e : typedterm)
| PatternMatch (e : typedterm) (pat : pattrn) (success failure : typedterm).
\end{verbatim}

Unfortunately this made us have 3 syntactic forms (untyped terms, typed terms and patterns) and it seemed to be too much for DbLib - the automatic facilities that help us prove the basic lemmas to make the library usable failed and I was also unable to fix the proofs manually.

So in the end we created a separate syntactic form for each pattern. This may look strange, but it is in fact equivalent to the previous formulation, it's just a technical / notational difference.

\begin{verbatim}
Inductive typedterm :=
| TypedTerm (u: term) (t: type)
with term :=
| Nat (n : nat)
| VAR (x : DeBruijnIndex)
| Lam (argT: type) (ebody : typedterm)
| App (e1 e2 : typedterm)
| Fix (e : typedterm)
| Lift (e: typedterm)
| Quote (e : typedterm)
| Splice (e : typedterm)
| MatchNat (e : typedterm) (n : nat) (es ef : typedterm)
| MatchVar (e : typedterm) (x : term) (es ef : typedterm)
| MatchApp (e : typedterm) (T1 T2 : type) (es ef : typedterm)
| MatchUnlift (e : typedterm) (es ef : typedterm)
| MatchLam (e : typedterm) (T : type) (es ef : typedterm)
| MatchFix (e : typedterm) (T : type) (es ef : typedterm).
\end{verbatim}

One other modification that we had to apply here is to the variable matching pattern. This pattern checks if the boxed code is a reference to some variable, so after the De Bruijn translation, this pattern also contains a De Bruijn index for the variable that is matched (see \texttt{PVar} in the first listing). This occurrence is unusual, because we need to update it when shifting all indices in a term, so that it always refers to the original variable (why we have to shift is explained in the previous section). It has to be treated differently, because it should never be substituted with some concrete value.

However in the library that we use, the \texttt{traverse} function is defined in such a way that during traversal we replace each variable with the result of calling \texttt{f l x} which handles all renaming and returns a new instance of the variable using the constructor from the \texttt{Var} typeclass. It is possible to overcome this by unpacking the result, checking if it indeed is an instance of \texttt{VAR} and getting the index from it, but this approach breaks the properties that the library requires of \texttt{traverse}.

Instead, we replace the raw De Bruijn index in \texttt{MatchVar} with an arbitrary term. This satisfies all the constraints required by the library and ensures that this index is updated when necessary. Unfortunately this also allows for ill-formed terms, where we do variable match over something other than a variable. This issue is however addressed in the typing rules - only instances of \texttt{MatchVar} in which \texttt{x} has the form \texttt{VAR y} are accepted. Thus, when we prove Preservation, we also prove that this variable is never substituted for (as the term resulting from such substitution would be ill-typed).

\subsection{Final form of the definitions}
\label{finalformencoding}
Most of syntactic details have already been described in the previous section.

Another important change from the paper version is the typing environment. On paper it is defined as a (partial) mapping from variable names to types and levels.
DbLib is also providing support for using environments with De Bruijn indices, so we used the approach suggested by the library authors - our environment is a list of options - such a list of options can be understood as a partial mapping from natural numbers and it integrates very well with operations on DeBruijn indices. For example when the environment is extended while going inside a binder, the new type is just prepended to the list - this both handles the new type and the fact that other indices are shifted.

We yet had to consider how to handle the levels in the typing environment. One possible solution could be to have a separate environment for both levels - so our \texttt{TypEnv} type would be \texttt{list (option type) * list (option type)}. However binders do not distinguish the levels that they bind, so this approach wouldn't work. Instead we store the level alongside the type in the mapping - so our \texttt{TypEnv} type is \texttt{list (option (level * type))}. The judgement from the original paper $\Gamma(x^i) = T$ is expressed as \texttt{lookup $x$ $\Gamma$ = Some (L$i$, $T$)}.

Extending the environment, ie. $\Gamma; x^i : T$ is expressed as \verb|insert x (Li, T) |$\Gamma$, where the new \verb|x| is the De Bruijn index corresponding to $x$. In the typing rules we only extend the environment by introducing a new binder which is the closest binder now (ie. its index will be 0) and all the other binder's indices are shifted by 1. This operation is expressed by \verb|insert 0 (Li, T) |$\Gamma$.

The typing judgement is defined as an inductive type \texttt{TypEnv $\to$ Level $\to$ typedterm $\to$ type $\to$ Prop} and the small-step semantics relation is defined as \texttt{Level $\to$ typedterm $\to$ typedterm $\to$ Prop}. 

We use Coq's Notations to make our judgements more easily readable. The typing judgement is expressed as \texttt{G $\vdash$(L) e $\in$ T} and the semantics relation as \texttt{e1 -->(L) e2} which quite closely resembles the notation used in the paper.

We also define a relation \texttt{evaluates} which is the reflexive transitive closure of small-step semantics at level 0 that is used in tests. For example we can define a term representing applying the identity function to a number and prove that it typechecks and evaluates correctly:
\begin{lstlisting}[mathescape=true]
Definition id_nat := (Lam TNat (VAR 0 : TNat) : TNat ==> TNat).
Definition id_applied := (App id_nat (Nat 42 : TNat) : TNat).
Lemma id_app_types : $\emptyset$ $\vdash$(L0) id_applied $\in$ TNat.
  repeat econstructor.
Qed.
Lemma id_app_evals : evaluates id_applied (Nat 42 : TNat).
  eapply star_step. repeat econstructor.
  unfold substitute. simpl_subst_all. auto.
Qed.
\end{lstlisting}



\section{Proving soundness}
\label{sec:proving}
We want to prove Progress: If $\emptyset \vdash^0 t \in T$, then $t$ is a value or there exists $t'$ such $t \longrightarrow^0 t'$
\\
and Preservation: If $\Gamma \vdash^i t \in T$ and $t \longrightarrow^i t'$, then $\Gamma \vdash^i t' \in T$.


First we had to reformulate the theorems into our language. Thanks to Coq's notations they don't differ much:
\begin{lstlisting}[mathescape=true]
Theorem Progress : forall t T,
  $\emptyset$ $\vdash$(L0) t $\in$ T ->
  isvalue t \/ exists t', t -->(L0) t'.
Theorem Preservation : forall t1 T G L,
  G $\vdash$(L) t1 $\in$ T ->
  forall t2,
  t1 -->(L) t2 ->
  G $\vdash$(L) t2 $\in$ T.
\end{lstlisting}

We have approached the proofs by first trying to prove a subset of the calculus corresponding to STLC, as a warm-up. Then we have added quotes, splices and lift and fixed the proofs. Later we have added the pattern matching and at the very end the fixpoint operator. Adding the features step-by-step required rewriting a substantial part of the proofs and sometimes even reformulating some basic definitions. But it also helped to keep the scope more manageable by dividing the proving effort into smaller pieces. The progressive approach also helped me learn piece-by-piece and better understand the paper proofs that I've been basing on.

\subsection{Induction}

As many proofs in programming language theory rely on induction on the structure of the term or the typing judgement, we needed appropriate induction principles.

One of the first challenges was that our definitions of terms and typedterms are mutually inductive. The default induction principles generated by Coq don't take this into account, so induction proofs using them are often too weak (the automatically generated principle doesn't have all the assumptions that are needed for it to be useful).

First we used Coq's \verb|Scheme| directive that allows to define induction principles that are mutually inductive, this already gives us an induction principle that has all the necessary assumptions. However, the induction principles require two properties that we will be proving - one for the typedterm and another for un-typed terms (which is of course expected, because in general we can't derive one from the other for arbitrary mutually inductive types). This is however not too convenient, because when using induction we have just one property we want to prove (usually for typedterms) and coming up with this dual version for un-typed terms for every instance of syntactic induction would be very cumbersome.

So to help with that, we define our own induction principle that combines the two in such a way, that it requires only a property we want to prove for typedterms and extends it to terms by ascribing to them an arbitrary type. We can prove its correctness using the principle automatically generated earlier.

Ascribing the terms with arbitrary types, which essentially means requiring it to hold for all types may sound like a too strong requirement. However in practice, all properties that we prove using this principle contain an assumption that the term is well-typed. Thus if the ascribed type is incorrect, the whole property holds trivially as it follows a false assumption. This is usually solved by the \verb|inversion| tactic on that typing assumption. Moreover we have proven the following lemma:
\begin{lstlisting}[mathescape=true]
Lemma AscribedTypeIsCorrect : forall G L t T T',
  G $\vdash$(L) (t : T) $\in$  T' -> T = T'.
\end{lstlisting}
which ensures that the type ascribed has to match the type resulting from the typing derivation.

\subsection{Progress}

The progress theorem in the original proof relied mostly on an auxiliary lemma - \textit{Level Progress}. It is defined in two parts, namely:
\\
For any given term $t$, we have:
\begin{itemize}
  \item[(1)]  If $\Gamma^{[1]} \vdash^0 t \in T$, then $t$ is a value or  there exists $t'$ such that $t \longrightarrow^0 t'$.
  \item[(2)] If $\Gamma^{[1]} \vdash^1 t \in T$ and $(\square t) : \square T$ is not a value,  then there exists $t'$ such that $t \longrightarrow^1 t'$.
\end{itemize}
Where $\Gamma^{[1]}$ means that the environment only contains level 1 variables.

Unsurprisingly, when proving by induction, each case needs the other. One approach to formulating this in Coq would be to define mutually recursive lemmas using the \verb|Lemma ... with| syntax. This is however quite prone to mistakes, because the fixpoint decreasing check is only done at the end.

Instead we formulate this as a single lemma with a disjunction:
\begin{lstlisting}[mathescape=true]
Lemma LevelProgress : forall t G T L,
  RestrictedLevel G L1 ->
  G $\vdash$(L) t $\in$ T ->
     (L = L0 /\ (isvalue t \/ exists t', t -->(L0) t')) 
  \/ (L = L1 /\ (not (isvalue (Quote t : $\square$T)) -> exists t', t -->(L1) t')).
\end{lstlisting}

This allows us to prove both cases in a single proof relying on syntactic induction on $t$ which thanks to the induction principle defined earlier provides us with all the necessary assumptions.

In that proof, in each case we \verb|destruct L| getting two cases where \verb|L| has a concrete value. In such case, in the inductive hypothesis's conclusion only one of the alternatives is true, so we proved two lemmas that allow us to simplify the inductive hypothesis to contain only the possible alternative.

The \textit{Level Progress} proof is quite long as, beside some trivial cases, each syntactic form required a slightly different treatment. However after proving that lemma, the \textit{Progress} theorem is a very simple corollary.

\subsection{Preservation}

To prove Preservation we need two auxiliary lemmas - \textit{Weakening} and \textit{Substitution}. These are closely related to handling variables in the environment, so they have to be quite deeply modified when translating to De Bruijn indices.

Weakening is originally defined as 
\\
If $\Gamma^{[1]} \vdash^i t \in T, x \not\in FV(\Gamma)$, then $\Gamma, x^j : T \vdash^i t \in T$.

The modification is rather intuitive, as explained in section \ref{finalformencoding}, extending the environment is defined using \verb|insert| and instead of a variable name $x$ we have an index \verb|x|. The fact that $x \not\in FV(\Gamma)$ is expressed by showing the property for \verb|shift x t| which shifts the indices inside of \verb|t| so that the \verb|x|th variable is not bound in that term, and other indices are modified in the same way as they were shifted by the \verb|insert|, so that they still correspond to each other. So it is defined as:
\begin{lstlisting}[mathescape=true]
Lemma Weakening: forall G L t T,
  G $\vdash$(L) t $\in$ T ->
  forall x L' T' G',
  insert x (L', T') G = G' ->
  G' $\vdash$(L) (shift x t) $\in$ T.
\end{lstlisting}
It is also important that we split the universal quantification in two parts. The lemma where all of the variables are defined in the first \verb|forall| clause would be equivalent. However when using the default induction principle, we would get too weak inductive hypotheses (because the \verb|x| etc. would be fixed instead of being universally quantified).

\paragraph{}

The Substitution lemma is a bit more complicated than usual:
\\
If (1) $\Gamma \vdash^j t_1 \in T_1$, (2) $\Gamma, x^j : T_1 \vdash^i t_2 \in T_2$ and
(3) $j = 0$ or $t_2$ does not contain pattern matches, then $\Gamma \vdash^i t_2[x \mapsto t_1] \in T_2$.

The first two conditions are as usual, the third one is a bit exceptional but it is necessary for the lemma to be provable.
The third condition says that we can substitute inside arbitrary terms for variables at level 0, but when substituting a variable at level 1, the term that we are substituting into cannot contain any pattern matches. Without this assumption, we may arrive at a situation when we are substituting a variable inside a pattern matching a particular variable (ie. $(t \sim x \; ? \; t_a \; || \; t_b)[x \mapsto t_c]$), we would get a result that is not even syntactically correct or typable. So we just disallow this kinds of situations (note that the variable pattern only applies to level 1 variables). At first it may seem like a too big restriction, making the lemma too weak. But in fact this is enough, because level 1 substitutions only happen when executing pattern matching and in pattern matches, we only allow to proceed with code values that contain plain terms (ie. terms without quotes, splices and pattern matches).

Once we understand the nuance about the third condition, translating the lemma into Coq is quite simple, so we skip it.

\paragraph{}

The Preservation proof also uses a few more quite obvious auxiliary lemmas to simplify some properties. For example we prove that a plain term doesn't contain pattern matches (which is almost by definition) or that renaming (ie. shifting indices in the De Bruijn formulation) doesn't introduce pattern matches if they weren't present originally. % As in the language's rules we also define a simplified version of substitution lemma that is closer to the notation used in the definitions and is a direct corollary of the main Substitution lemma.

We extensively use the tactics provided by DbLib for dealing with substitutions and define some more tactics specific to our formulation that help simplifying substitutions in the proof context.
\section{Discussion}
\label{sec:discussion}
\subsection{Multiple binders at once}
\label{multibind}

\todo[inline]{TODO: describe 2 binders in App issue}

\subsection{Variable binding libraries}
\label{binderlibs}

As handling variable binding is a quite complex but very common problem for all programming language formalizations, there are some libraries that help dealing with it. Below we describe three libraries that we approached and our experiences using them.

\paragraph{}

\textit{autosubst} \cite{autosubst} is quite simple to use. We use special types when defining the syntactic forms: a separate type \verb|var| for the De Bruijn index which is an alias for \verb|nat| but it is tagged so that the library knows this is an index and handles it properly; and a type constructor \verb|bind| that wraps any type, indicating that anything inside it consumes one binder. All properties are derived automatically. A big advantage of this library is that its simplification tactics are quite powerful, at least in our experience they worked out of the box most of the time. It is a very user-friendly and robust solution for simple calculi.

Unfortunately, \textit{autosubst} was too simple for our use-case. In theory it supports having two types of syntax forms, but we had issues with using it. It also only allows binding one variable per syntactic form whereas we need to bind at least two (for \verb|MatchApp|). It may be possible to work around this issue by introducing some dummy syntactic forms.

% It could be possible to work around this issue by introducing an intermediate syntactic form so that \verb|MatchApp| is divided into two forms, ie \verb|MatchApp e T1 T2 es ef| could become \verb|MatchApp1 e T1 T2 (MatchApp2 es) ef| (because \verb|es| is the term that needs to bind two variables). We didn't test if this would work, because before coming up with it we tried a different library, but it would be a rather 'hacky' approach.

\paragraph{}

We chose to use \textit{DbLib} \cite{dblib}. It is a bit harder to start with but is much more flexible. Instead of auto-generating how the binders behave (like \textit{autosubst} did), the user defines the syntactic forms and a \verb|traverse| function which describes how the binders behave (the \verb|traverse| function is described in section \ref{traverse}). This allows for having multiple bindings. The library also supports having multiple syntactic forms, although when using that feature it may be necessary to manually call the tactics that automate derivation of basic properties. For more complex syntactic structures, these tactics failed, so we had to keep the structure quite simple. The tactics for simplifying substitution were a bit less robust than in \textit{autosubst} and sometimes needed some help from the user, but it seems reasonable given that this library allows for more complicated syntax definitions.

\paragraph{}

Another approach worth mentioning is \textit{Locally Nameless} \cite{locallynameless}. It is more of an approach than a library (although the author shares a library with useful tactics). It is a hybrid approach that uses De Bruijn indices for bound variables but uses some arbitrary naming (as one would in a normal programming language) for the free variables (so the syntax needs two forms for variables - one for bound variables and another one for free variables). Using De Bruijn indices for bound variables makes it easier to manually define the substitution operation (whereas in the other libraries it was derived automatically) which is split into two operations - there is one function for substituting free variables and separate one for substituting the first DeBruijn index (which is called \textit{opening} the term). 

It seems to be the most flexible approach but also the most complex one, as the user has to do most of the work and the amount of automation provided by the library is rather limited. It is also based on quite complicated principles (De Bruijn indices are hard to understand on their own and mixing them with normal names makes it even more difficult). It is a good choice for complex calculi for which the other libraries would be too limited.

\paragraph{}

In this project, the choice of the library was quite highly influenced by my level of understanding. I started with \textit{autosubst} which was the simplest one and quite easy to learn, but than it seemed that it imposed some constraints that didn't allow to express important parts of the calculus (although it may have been possible to work around them with some clever and ugly tricks). Than I tried both \textit{Locally Nameless} and \textit{DbLib}, but at that point \textit{Locally Nameless} was still too hard to understand it to be able to use it. \textit{DbLib} was a nice middle-ground between flexibility and complexity and it still offered decent automation. After using it and understanding better more nuances of using De Bruijn indices, I can appreciate the beauty of \textit{Locally Nameless} approach, but given its limited amount of automation, it is a library best used for more complex calculi.

\subsection{Testing before proving}
Adding the pattern matching was a very delicate matter, especially as we had to diverge slightly from the original calculus. It was easy to make mistakes in the definitions of types and syntax (for example one of the bugs was having the two variables bound by app in different order in the typing judgement and in the semantics). Such mistakes lead to unprovable theorems, but they may not be easy to notice during proving. To avoid such issues, I decided to write some `unit tests' - I defined simple programs in the calculus that I know should work and tried proving that they do typecheck and evaluate correctly. These proofs were usually rather easy but they helped find lots of simple mistakes. Moreover, they can serve as examples of how to use the calculus in the formalized version which helps understanding.

It seems like a very good approach to write some simple tests (for example lemmas about the desired properties of some specific simple terms, like the ones at the end of section \ref{finalformencoding}) before moving on to the general soundness proofs.



\subsection{Proof stability, naming}

Another good practice that I learnt along the way is trying to make the proofs resistant to changes. It is quite common to have to add some additional assumption in one of the definitions or simply extend the calculus with new definitions. If we don't write our proofs carefully, a small modification of the definitions may require us to rewrite most of them, while writing them well may result in not having to change anything.

Firstly, it's good to use as much automation as possible, as it is usually robust to small changes. When introducing hypotheses (with \verb|intro| or \verb|assert|) it is good to give them names, because the automatic naming may very easily change if an additional assumption is added in the future.

In big case-by-case proofs (for example syntactic induction), I usually try to come up with some basic strategy that solves at least the simple goals (using \verb|induction X; try solve [general strategy].|), but in more complex proofs we may still have to manually approach many goals. It is then a good idea when focusing a new goal to write a comment shortly describing the current goal. This helps when by adding some additional rules, the order of the goal changes which may result in applying tactics to wrong goals and very confusing errors. These comments help to quickly catch situations when the current goal is different than would be expected.

I didn't find a way to name hypotheses introduced by calling the \verb|inversion| tactics (which I use extensively), so I still had to deal with the tactics having unpredictable names. It is also problematic when combining tactics. For example in the chain \verb|induction t; inversion H1.|, \verb|H1| may introduce different sets of hypotheses for each case. Sometimes I also want to call \verb|inversion| but the particular hypothesis that I want to examine will be called differently in different branches of a tactic chain similar to the one just mentioned. The solution to these issues is matching the goal - the ability to write tactics that match the current goal or available hypotheses against some patterns. For example we can write:
\begin{lstlisting}[mathescape=true]
Ltac invV :=
  match goal with
  | H: ?G $\vdash$(L0) ?v $\in$ $\square$(?T) |- _ => inversion H; subst
  end.
\end{lstlisting}
which defines a tactic that finds a hypothesis containing a typing judgement for some quoted type (a $\square T$ for some $T$) and runs \verb|inversion| on this hypothesis. Now we can write \verb|induction t; invV.| and regardless of what that hypothesis would be called, we can find it and inside of \verb|invV| bind its name to \verb|H|. This is a very powerful technique that allows to write very useful tactics.


\section{Summary}
\label{sec:summary}

We have described the $\lambda^{\RIGHTcircle}$ calculus and various challenges concerning formalizing it in Coq and proving its soundness. We discuss some of the available libraries useful in formalizing calculi and some other good practices for proving in Coq. Our main contribution is the mechanized formalization of the calculus along with its soundess proofs that can be found at \href{https://github.com/radeusgd/QuotedPatternMatchingProof/}{https://github.com/radeusgd/QuotedPatternMatchingProof/}.

\bibliographystyle{unsrt}
\bibliography{bibliography}


\section*{Appendix}
\def\e{e}
\def\t{t}
\def\u{u}
\def\p{p}

\newcommand{\quoted}[1]{\Box\;#1}
\newcommand{\lift}[1]{\texttt{lift}\;#1}
\newcommand{\qtype}[1]{\Box#1}
\newcommand{\splice}[1]{\$\;#1}
\newcommand{\fix}[1]{\texttt{fix}\;#1}
\newcommand{\app}[2]{#1\;#2}
\newcommand{\tpd}[2]{{#1}{:}{#2}}

\newcommand{\patnat}[1]{#1}
\newcommand{\patapp}[2]{#1\;#2}
\newcommand{\patref}[1]{#1}
\newcommand{\patunlift}[1]{\texttt{unlift}\;#1}
\newcommand{\patbind}[2]{\texttt{bind}[#2]\;#1}
\newcommand{\patlam}[2]{\texttt{lam}[#2]\;#1}
\newcommand{\patfix}[1]{\texttt{fix}\;#1}

\newcommand{\patmat}[4]{\patmatSP{#1}{#2}\patmatThen{#3}#4}
\newcommand{\patmatSP}[2]{#1 \sim #2\;?}
\newcommand{\patmatThen}[1]{\;#1\; \| \;}

\newcommand{\lam}[3]{\lambda#1{:}#2.#3}

\newcommand{\strip}[1]{|#1|}


To simplify the proofs, we remove nested patterns leaving only simpler non-nested pattern matching. This requires changing some typing and semantic rules.

As we use De Bruijn indices, the rules in the formalization look slightly different. For readability we present them in similar style to the original, so we introduce names for bound variables and add them in the syntax of the pattern matches. $b_0$ stands for the introduced variable name that corresponds to the most closely bound De Bruijn index and $b_1$ stands for the next index.

For clarity, we show how the syntax of our patterns corresponds to non-nested patterns in the original syntax in figure \ref{fig:pattern-syntax}.

\newcommand{\syntaxeq}[2]{ #1 \quad &\equiv \quad #2 \\ }

\begin{figure}[h]
  \centering
  \begin{framed}
    \begin{align}
     \syntaxeq
     { \texttt{MatchNat} \; t_1 \; n \; t_2 \; t_3 }
     { \patmat{t_1}{\patnat{n}}{t_2}{t_3} }     
    \syntaxeq
     { \texttt{MatchVar} \; t_1 \; x \; t_2 \; t_3 }
     { \patmat{t_1}{\patref{x}}{t_2}{t_3} }
         \syntaxeq
     { \texttt{MatchApp} \; t_1 \; T_1 \; T_2 \; b_0 \; b_1 \; t_2 \; t_3 }
     { \patmat{t_1}{\patapp{(\patbind{b_0}{T_1})}{(\patbind{b_1}{T_1 \to T_2})}}{t_2}{t_3} }     
         \syntaxeq
     { \texttt{MatchUnlift} \; t_1 \; b_0 \; t_2 \; t_3 }
     { \patmat{t_1}{\patunlift{b_0}}{t_2}{t_3} }  
         \syntaxeq
     { \texttt{MatchLam} \; t_1 \; (T_1 \to T_2) \; b_0 \; t_2 \; t_3 }
     { \patmat{t_1}{\patlam{b_0}{T_1 \to T_2}}{t_2}{t_3} }
     \syntaxeq
     { \texttt{MatchFix} \; (\tpd{t_1}{T_1}) \; b_0 \; t_2 \; t_3 }
     { \patmat{(\tpd{t_1}{T_1})}{\patfix{(\patbind{b_0}{T_1 \to T_1})}}{t_2}{t_3} }
     \end{align}
  \end{framed}
  \caption{Pattern syntax clarification}
  \label{fig:pattern-syntax}
\end{figure}

As we have separate syntactic forms for each pattern, we remove the general \textsc{T-Pat} rule and its helper pattern typing rules and we replace them with rules outlined in figure \ref{fig:pattern-type-system}. The rules that were kept are listed in \ref{fig:explicitly-typed-type-system}.

\begin{figure}
  \centering
  \begin{framed}
    
    \textbf{Explicitly-typed Terms Typing} \hfill \framebox[1.2\width][r]{$\Gamma \vdash^i t \in T$}
    
    \begin{multicols}{2}
      
      % Coll 1
      
      \vbox{
        \infax[T-Nat]
        { \Gamma \vdash^i \tpd{n}{Nat} \in Nat  }
        \vspace{2mm}
      }
      
      \infrule[T-Var]
      { \Gamma(x^i) = T }
      { \Gamma \vdash^i \tpd{x}{T} \in T  }
      
      \vspace{1mm}
      
      \infrule[T-Abs]
      { \Gamma,x^i{:}T_1 \vdash^i \t_2 \in T_2 }
      { \Gamma \vdash^i \tpd{(\lam{x}{T_1}{\t_2})}{T_1{\to}T_2} \;\in\; T_1{\to}T_2  }
      
      \vspace{1mm}
      
      \infrule[T-App]
      { \Gamma \vdash^i \t_1 \in T_1{\to}T_2 \andalso  \Gamma \vdash^i \t_2 \in T_1}
      { \Gamma \vdash^i \tpd{(\app{\t_1}{\t_2})}{T_2} \;\in\; T_2  }
      
      
      % Coll 2
      
      \infrule[T-Fix]
      { \Gamma \vdash^i \t \in T{\to}T }
      { \Gamma \vdash^i \tpd{(\fix{\t})}{T} \;\in\; T  }
      
      \vspace{1mm}
      
      \infrule[T-Lift]
      { \Gamma \vdash^0 \t \in Nat }
      { \Gamma \vdash^0 \tpd{(\lift{t})}{\qtype{Nat}} \;\in\; \qtype{Nat} }
      
      \vspace{1mm}
      
      \infrule[T-Box]
      { \Gamma \vdash^1 \t \in T }
      { \Gamma \vdash^0 \tpd{(\quoted{\t})}{\qtype{T}} \;\in\; \qtype{T} }
      
      \vspace{1mm}
      
      \infrule[T-Unbox]
      { \Gamma \vdash^0 \t \in \qtype{T} }
      { \Gamma \vdash^1 \tpd{(\splice{\t})}{T} \;\in\; T  }
      
      
    \end{multicols}
    
  \end{framed}
  \caption{Explicitly-typed Terms Typing (retained rules)}
  \label{fig:explicitly-typed-type-system}
\end{figure}

\begin{figure}
\centering
\begin{framed}
    
    \textbf{Explicitly-typed Terms Typing (added rules)} \hfill \framebox[1.2\width][r]{$\Gamma \vdash^i t \in T$}

    \infrule[T-Pat-Nat]
{
  \Gamma \vdash^0 t_1 \in \qtype{Nat} \andalso
  \Gamma \vdash^0 t_2 \in T \andalso
  \Gamma \vdash^0 t_3 \in T
}
{ \Gamma \vdash^0 \tpd{(\patmat{t_1}{\patnat{n}}{t_2}{t_3})}{T} \;\in\; T  }

\vspace{1mm}

\infrule[T-Pat-Var]
{
  \Gamma \vdash^0 t_1 \in \qtype{T_1} \andalso
  \Gamma(x^1) = T_1 \andalso
  \Gamma \vdash^0 t_2 \in T \andalso
  \Gamma \vdash^0 t_3 \in T
}
{ \Gamma \vdash^0 \tpd{(\patmat{t_1}{\patref{x}}{t_2}{t_3})}{T} \;\in\; T  }

\vspace{1mm}

\infrule[T-Pat-App]
{
  \Gamma \vdash^0 t_1 \in \qtype{T_2} \andalso
  \Gamma; \, b_1^0 : \qtype{(T_1 \to T_2)}; \, b_0^0 : \qtype{T_1} \vdash^0 t_2 \in T \andalso
  \Gamma \vdash^0 t_3 \in T
}
{ \Gamma \vdash^0 \tpd{(\patmat{t_1}{\patapp{(\patbind{b_0}{T_1})}{(\patbind{b_1}{T_1 \to T_2})}}{t_2}{t_3})}{T} \;\in\; T  }

\vspace{1mm}

\infrule[T-Pat-Unlift]
{
  \Gamma \vdash^0 t_1 \in \qtype{Nat} \andalso
  \Gamma; \, b_0^0 : Nat \vdash^0 t_2 \in T \andalso
  \Gamma \vdash^0 t_3 \in T
}
{ \Gamma \vdash^0 \tpd{(\patmat{t_1}{\patunlift{b_0}}{t_2}{t_3})}{T} \;\in\; T  }

\vspace{1mm}

\infrule[T-Pat-Lam]
{
  \Gamma \vdash^0 t_1 \in \qtype{T_1 \to T_2} \andalso
  \Gamma; \, b_0^0 : \qtype{T_1} \to \qtype{T_2} \vdash^0 t_2 \in T \andalso
  \Gamma \vdash^0 t_3 \in T
}
{ \Gamma \vdash^0 \tpd{(\patmat{t_1}{\patlam{b_0}{T_1 \to T_2}}{t_2}{t_3})}{T} \;\in\; T  }

\vspace{1mm}

\infrule[T-Pat-Fix]
{
  \Gamma \vdash^0 t_1 \in \qtype{T_1} \andalso
  \Gamma; \, b_0^0 : \qtype{(T_1 \to T_1)} \vdash^0 t_2 \in T \andalso
  \Gamma \vdash^0 t_3 \in T
}
{ \Gamma \vdash^0 \tpd{(\patmat{(\tpd{t_1}{T_1})}{\patfix{(\patbind{b_0}{T_1 \to T_1})}}{t_2}{t_3})}{T} \;\in\; T  }

\end{framed}
\caption{Explicitly-typed Terms Typing (added rules)}
\label{fig:pattern-type-system}
\end{figure}

Similarly, we need separate \textsc{E-Pat-Succ}, \textsc{E-Pat-Fail} and \textsc{E-Pat} for each pattern as they are separate syntactic forms. This however makes the \textit{match} function obsolete, as all matching rules are directly expressed by these separate rules, as listed in figure \ref{fig:pattern-semantics}. The other semantic rules that were kept are listed in figure \ref{fig:operational-semantics}.

\begin{figure}
  \centering
  \begin{framed}
    
    \begin{minipage}{0.45\linewidth}
      \infrule[E-App-1]
      { \t_1 \longrightarrow^i \t_1' }
      { \tpd{(\app{\t_1}{\t_2})}{T} \longrightarrow^i \tpd{(\app{\t_1'}{\t_2})}{T} }
      
      \vspace{5mm}
      
      \infrule[E-App-2]
      { \t_2 \longrightarrow^i \t_2' }
      { \tpd{(\app{\t_1}{\t_2})}{T} \longrightarrow^i \tpd{(\app{\t_1}{\t_2'})}{T} }
      
      \vspace{5mm}
      
      \infrule[E-Abs]
      { \t \longrightarrow^1 \t' }
      { \tpd{(\lam{x}{T_1}{\t})}{T} \longrightarrow^1 \tpd{(\lam{x}{T_1}{\t'})}{T} }
      
      \vspace{5mm}
      
      \infax[E-Lift-Red]
      { \tpd{(\lift{n})}{T} \longrightarrow^0 \tpd{(\quoted{n})}{T} }
      
    \end{minipage}
    \begin{minipage}{0.50\linewidth}
      
      \infrule[E-Box]
      { \t \longrightarrow^1 \t' }
      { \tpd{(\quoted{\t})}{T} \longrightarrow^0 \tpd{(\quoted{\t'})}{T} }
      
      \vspace{5mm}
      
      \infrule[E-Unbox]
      { \t \longrightarrow^0 \t' }
      { \tpd{(\splice{\t})}{T} \longrightarrow^1 \tpd{(\splice{\t'})}{T} }
      
      \vspace{8mm}
      
      \infax[E-Splice]
      { \tpd{(\splice{\quoted{\hat\t\,}})}{T} \longrightarrow^1 \tpd{\hat\t}{T} }
      
      \vspace{5mm}
      
      \infrule[E-Lift]
      { \t \longrightarrow^0 \t' }
      { \tpd{(\lift{\t})}{T} \longrightarrow^0 \tpd{(\lift{\t'})}{T} }
      
    \end{minipage}
    
    \vspace{4mm}
    
    \infax[E-Beta]
    { \tpd{(\app{\tpd{(\lam{x}{T_1}{\t})}{(T_1{\to}T_2})}{v})}{T_2} \longrightarrow^0 \t[x \mapsto \strip{v}] }
    
    \vspace{1mm}
    
    \infrule[E-Fix]
    { \t \longrightarrow^i \t' }
    { \tpd{(\fix{\t})}{T} \longrightarrow^i \tpd{(\fix{\t'})}{T} }
    
    \vspace{1mm}
    
    \infax[E-Fix-Red]
    { \tpd{(\fix{\lam{f}{T}{\t})}}{T} \longrightarrow^0 \t[f \mapsto \fix{\lam{f}{T}{\t}}] }
    
  \end{framed}
  \caption{Small-step operational semantics (retained rules)}
  \label{fig:operational-semantics}
\end{figure}


\begin{figure}
  \centering
  \begin{framed}
  
\infrule[E-PatNat-Red]
{ t \longrightarrow^0 t'}
{ \tpd{(\patmat{t}{\patnat{n}}{t_2}{t_3})}{T} \longrightarrow^0 \tpd{(\patmat{t'}{\patnat{n}}{t_2}{t_3})}{T} }

\infax[E-PatNat-Succ]
{ \tpd{(\patmat{\tpd{(\quoted{n})}{\qtype{Nat}}}{\patnat{n}}{t_2}{t_3})}{T} \longrightarrow^0 t_2 }

\infrule[E-PatNat-Fail]
{ t \not= n \andalso t \text{ is plain} }
{ \tpd{(\patmat{\tpd{(\quoted{t})}{\qtype{Nat}}}{\patnat{n}}{t_2}{t_3})}{T} \longrightarrow^0 t_3 }

\vspace{1mm}

\infrule[E-PatVar-Red]
{ t \longrightarrow^0 t' }
{ \tpd{(\patmat{t}{\patref{x}}{t_2}{t_3})}{T} \longrightarrow^0 \tpd{(\patmat{t'}{\patref{x}}{t_2}{t_3})}{T} }

\infax[E-PatVar-Succ]
{ \tpd{(\texttt{MatchVar} \; \tpd{(\quoted{x})}{\qtype{T_1}} \; x \; t_2 \; t_3)}{T} \longrightarrow^0 t_2 }

\infrule[E-PatVar-Fail]
{ t \not= x \andalso t \text{ is plain} }
{ \tpd{(\texttt{MatchVar} \; \tpd{(\quoted{x})}{\qtype{T_1}} \; x \; t_2 \; t_3)}{T} \longrightarrow^0 t_3 }

\vspace{1mm}

\infrule[E-PatApp-Red]
{ t \longrightarrow^0 t'}
{ \tpd{(\patmat{t}{\patapp{(\patbind{b_0}{t})}{(\patbind{b_1}{t \to T_2})}}{t_2}{t_3})}{T} \longrightarrow^0 \tpd{(\patmat{t'}{\patapp{(\patbind{b_0}{t})}{(\patbind{b_1}{t \to T_2})}}{t_2}{t_3})}{T} }

\infrule[E-PatApp-Succ]
{ t = (e_1 \; e_2) \andalso t \text{ is plain} }
{ \tpd{(\texttt{MatchApp} \; \tpd{(\quoted{t})}{\qtype{T_3}} \; T_1 \; T_2 \; b_0 \; b_1 \; t_2 \; t_3)}{T} \longrightarrow^0 t_2[b_0 \mapsto \quoted{e_2}][b_1 \mapsto \quoted{e_1}] }

\infrule[E-PatApp-Fail]
{ t \text{ is not an application} \andalso t \text{ is plain} }
{ \tpd{(\texttt{MatchApp} \; \tpd{(\quoted{t})}{\qtype{T_3}} \; T_1 \; T_2 \; b_0 \; b_1 \; t_2 \; t_3)}{T} \longrightarrow^0 t_3 }


\vspace{1mm}

\infrule[E-PatUnlift-Red]
{ t \longrightarrow^0 t'}
{ \tpd{(\patmat{t}{\patunlift{b_0}}{t_2}{t_3})}{T} \longrightarrow^0 \tpd{(\patmat{t'}{\patunlift{b_0}}{t_2}{t_3})}{T} }

\infax[E-PatUnlift-Succ]
{ \tpd{(\texttt{MatchUnlift} \; \tpd{(\quoted{n})}{\qtype{Nat}} \; b_0 \; t_2 \; t_3)}{T} \longrightarrow^0 t_2[b_0 \mapsto n] }

\infrule[E-PatUnlift-Fail]
{ t \text{ is not a number} \andalso t \text{ is plain} }
{ \tpd{(\texttt{MatchUnlift} \; \tpd{(\quoted{t})}{\qtype{T_1}} \; b_0 \; t_2 \; t_3)}{T} \longrightarrow^0 t_3 }

\vspace{1mm}

\infrule[E-PatLam-Red]
{ t \longrightarrow^0 t'}
{ \tpd{(\patmat{t}{\patlam{b_0}{t \to T_2}}{t_2}{t_3})}{T} \longrightarrow^0 \tpd{(\patmat{t'}{\patlam{b_0}{t \to T_2}}{t_2}{t_3})}{T} }

\infrule[E-PatLam-Succ]
{ t = \lam{x}{T_1}{e} \andalso t \text{ is plain} \andalso x' \text{ is fresh} }
{ \tpd{(\texttt{MatchLam} \; \tpd{(\quoted{t})}{\qtype{(T_1 \to T_2)}} \; (T_1 \to T_2) \; b_0 \; t_2 \; t_3)}{T} \longrightarrow^0 t_2[b_0 \mapsto \lam{x'}{\qtype{T_1}}{\quoted{e[x \mapsto \splice{x'}]}}] }

\infrule[E-PatLam-Fail]
{ t \text{ is not a lambda expression} \andalso t \text{ is plain} }
{ \tpd{(\texttt{MatchLam} \; \tpd{(\quoted{t})}{\qtype{T_1 \to T_2}} \; T_1 \; b_0 \; t_2 \; t_3)}{T} \longrightarrow^0 t_3 }

\vspace{1mm}

\infrule[E-PatFix-Red]
{ t \longrightarrow^0 t'}
{ \tpd{(\texttt{MatchFix} \; t \; b_0 \; t_2 \; t_3)}{T} \longrightarrow^0 \tpd{(\texttt{MatchFix} \; t' \; b_0 \; t_2 \; t_3)}{T} }

\infrule[E-PatFix-Succ]
{ t = \fix{e} \andalso t \text{ is plain} }
{ \tpd{(\texttt{MatchFix} \; \tpd{(\quoted{t})}{\qtype{T_1}} \; b_0 \; t_2 \; t_3)}{T} \longrightarrow^0 t_2[b_0 \mapsto \quoted{e}] }

\infrule[E-PatFix-Fail]
{ t \text{ is not a fix expression} \andalso t \text{ is plain} }
{ \tpd{(\patmat{(\tpd{t}{t})}{\patfix{(\patbind{b_0}{t \to t})}}{t_2}{t_3})}{T} \longrightarrow^0 t_3 }

  
  \end{framed}
  \caption{Small-step operational semantics (added rules)}
  \label{fig:pattern-semantics}
\end{figure}


$\forall_e. t \not= \fix{e}$

\end{document}