% Template by:
% for 18/19 Trimester 2
% Chia Jason 1161300548
% Hor Sui Lyn 1161300122
%
\documentclass[runningheads]{article}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage[fleqn]{amsmath}
\usepackage{stmaryrd}
\usepackage{listings}
%\usepackage{minted}
\usepackage[dvipsnames]{xcolor}
\usepackage[hidelinks]{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{pxfonts}
\usepackage{todonotes}
\usepackage{wasysym} % lambda circle symbol
%\usepackage{languages}
\usepackage{bcprules}     % typeset inference rule
\usepackage{multicol}     % multi-column

\usepackage{framed}


%\newtheorem*{theorem-no-num}{Theorem}

\setlength{\marginparwidth}{2cm} %% TODO remove - used for TODO
\newcommand{\calculus}{$\lambda^{\RIGHTcircle}$} 
\lstset{basicstyle=\ttfamily}
%\lstdefinestyle{pdot}{
%  %numbers=left,
%  %stepnumber=1,
%  %numbersep=10pt,
%  %tabsize=4,
%  showspaces=false,
%  showstringspaces=false,
%  mathescape=true,
%  breaklines=true,
%  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},linewidth=1.1\textwidth,
%  %xleftmargin=-0.1\textwidth
%}
%\lstset{basicstyle=\small\ttfamily}

% \newcommand{\fresh}[1]{\boxed{#1}}

\def\e{e}
\def\t{t}
\def\u{u}
\def\p{p}

\newcommand{\quoted}[1]{\Box\;#1}
\newcommand{\lift}[1]{\texttt{lift}\;#1}
\newcommand{\qtype}[1]{\Box#1}
\newcommand{\splice}[1]{\$\;#1}
\newcommand{\fix}[1]{\texttt{fix}\;#1}
\newcommand{\app}[2]{#1\;#2}
\newcommand{\tpd}[2]{{#1}{:}{#2}}

\newcommand{\patnat}[1]{#1}
\newcommand{\patapp}[2]{#1\;#2}
\newcommand{\patref}[1]{#1}
\newcommand{\patunlift}[1]{\texttt{unlift}\;#1}
\newcommand{\patbind}[2]{\texttt{bind}[#2]\;#1}
\newcommand{\patlam}[2]{\texttt{lam}[#2]\;#1}
\newcommand{\patfix}[1]{\texttt{fix}\;#1}

\newcommand{\patmat}[4]{\patmatSP{#1}{#2}\patmatThen{#3}#4}
\newcommand{\patmatSP}[2]{#1 \sim #2\;?}
\newcommand{\patmatThen}[1]{\;#1\; \| \;}

\newcommand{\lam}[3]{\lambda#1{:}#2.#3}

\newcommand{\strip}[1]{|#1|}


%\lstset{emph={%  
%    val, match, case%
%  },emphstyle={\bfseries}%
%}%
\lstdefinelanguage{scala}{
  morekeywords={abstract,case,catch,class,def,%
    do,else,extends,false,final,finally,%
    for,if,implicit,import,match,mixin,%
    new,null,object,override,package,%
    private,protected,requires,return,sealed,%
    super,this,throw,trait,true,try,%
    type,val,var,while,with,yield},
  otherkeywords={=>,<-,<\%,<:,>:,\#,@},
  sensitive=true,
  morecomment=[l]{//},
  morecomment=[n]{/*}{*/},
  morestring=[b]",
  morestring=[b]"""
}

\lstdefinestyle{myScalastyle}{
  frame=tb,
  language=scala,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{green},
  stringstyle=\color{red},
  frame=single,
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
}

\newtheorem*{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}


\begin{document}
% title
\title{A Mechanized Theory of Quoted Patterns}
%\thanks{Supervised by} % if we want to thank someone/org.
%
% authors
\author{Radosław Waśko}
%\author{Chia Jason\inst{1}\orcidID{0000-0002-2056-1687} \and
%Hor Sui Lyn\inst{1}\orcidID{0000-0003-2614-374X}}

%
%\authorrunning{R. Waśko}
% First names are abbreviated in the running head.
%
%\institute{{EPFL}}
%
\maketitle              % typeset the header of the contribution
%formal
\begin{abstract}
  %%% https://plg.uwaterloo.capaper/~migod/research/beckOOPSLA.html
The pattern matching on code from the new macro system of Scala 3 is modelled by a calculus called \calculus. We present a mechanized proof of soundness of the calculus in Coq and discuss encountered challenges.
%
%\keywords{Attendance System  \and Parameter tampering \and Exploit discovery.}
\end{abstract}
%
%
\section{Introduction}

Scala 3 is getting a new macro system based on staged computation \cite{dottydocMacro} which also supports pattern matching over code values. The theory behind the staged computation is described in the paper \textit{A Theory of Quoted Code Patterns} \cite{QPM}. 

For example, one can quote an expression and pattern match over it to transform it \footnote{Example from \cite{QPM}}

\begin{lstlisting}[style=myScalaStyle]
val code = '{ println(1+2) }
code match {
  case '{ println($x)) } =>
    '{ println("Result of " + ${Expr(x.show)} + ": " + $x) }
}
\end{lstlisting}

The code above will print: \texttt{Result of 1+2: 3}.

The language features are modelled by \calculus calculus which is an extension of \textit{simply typed lambda calculus} with quotes, splices and pattern matching over quoted code values. The calculus is explained in more detail in section \ref{sec:calculus}.

The goal of this project was to create a mechanized proof of the soundness of \calculus, based on the paper proofs in the original paper. The project consists of 1366 lines of Coq code\footnote{The code can be found at  \href{https://github.com/radeusgd/QuotedPatternMatchingProof/}{https://github.com/radeusgd/QuotedPatternMatchingProof/}}, of which 585 are proofs and 455 are definitions. The definitions are explained in section \ref{sec:definitions} and the proofs in section \ref{sec:proving}.

While working on the proofs, we learned some lessons that may be useful in future work. We discuss these in section \ref{sec:discussion}.

\section{\calculus calculus}
\label{sec:calculus}

The system is formalized in two parts --- there is an \textit{implicitly-typed} language and an \textit{explicitly-typed} variant in which every term is ascribed with its type. There are elaboration rules for converting \textit{implicitly-typed} terms into \textit{explicitly-typed}.

There are three kinds of types: a base type for natural numbers ($Nat$), a function type ($T_1 \to T_2$) and a type for code values ($\square T$).

The syntax contains the usual constructs from STLC: lambda abstractions, applications and variables. Moreover, it has natural number constants and a fixpoint operator to support recursion.

It is then extended with quote ($\square e$) and splice ($\$e$) operators typical for staged computation, a \verb|lift| operator to lift a numeric value to a code value returning that number, and a simple pattern match construct ($e \sim p \; ? \; e_{succ} \; || \; e_{fail}$).
The pattern match checks if $e$ matches the pattern $p$: if it does, the expression reduces to $e_{succ}$ where any bindings introduced by the pattern are available, otherwise it reduces to $e_{fail}$.

A non-reducible value in this calculus can be a natural number, a lambda or a quoted plain code value. 

A plain term is defined as the non-staged subset of the calculus --- lambdas, abstractions, constants and fixpoint expressions. Quotes, splices and pattern matching are excluded in plain terms.

Nesting of quotes and splices is regulated by \textit{levels} - level 0 terms are the top-level terms that are executed and level 1 terms are boxed code (which can also contain level 0 terms by splicing). For simplicity, the calculus only supports two levels (so it is not possible to have code values containing code values).

Below is an example of a term in the calculus. The code implements a function that matches application of a function taking a number and replaces its argument with $42$. If the match fails the original code is returned.

\begin{equation*}
  \Big(\lam{e}{\qtype{Nat}}{e \sim \big( \patapp{\patbind{f}{Nat \to Nat}}{\patbind{x}{Nat}} \big) \; ? \; \square(\$f \; 42) \; || \; e} \Big) \;\; \square((\lam{x}{Nat}{x}) \; 10)
\end{equation*}

This term will match the suspended function application and replace the argument, thus it reduces to $\square((\lam{x}{Nat}{x}) \; 42)$.

\section{Definitions in Coq}
\label{sec:definitions}
We define the \textit{explicitly-typed} language, as the safety properties are defined for it and the elaboration of the \textit{implicitly-typed} language is quite simple.

Defining the calculus in Coq requires some simplifications. The simplifications don't limit the expressivity of the language.

\subsection{Variable Binding}

Variable binding is a common issue in the mechanization of calculi.

On paper we usually use textual names and define a capture-avoiding substitution operation. This is possible in Coq, but this kind of function is not so easy to define. We need to apply recursively on terms that are not structurally sub-terms of the main terms (because sometimes they may be renamed etc.). As Coq only accepts total functions, we need to show that the invocations are indeed on terms smaller in some way. Moreover, the concept of $\alpha$-\textit{equivalence} becomes much more complicated because one has to define properties in terms of equivalence classes of terms. This highly complicates the definitions and also makes it harder to reason about theorems.

\subsubsection{De Bruijn indices}

The usual solution to this problem is using De Bruijn indices \cite{Bruijn1972LambdaCN}. In a term encoded using De Bruijn indices, variable names are replaced by indices --- numbers that indicate \textit{binder distance} (how many binders are in scope in the expression tree between the variable and the binder that this variable is referring to). In such notation, $\alpha-\text{equivalence}$ is reduced to syntactic equality and defining capture avoiding substitution is much simpler.

For example, the identity function $\lambda x. \, x$ is encoded as $\lambda. \, \#0$ and the K combinator ${\color{blue}\lambda x}. \, {\color{OliveGreen}\lambda y}. \, {\color{blue}x}$ becomes ${\color{blue}\lambda}. \, {\color{OliveGreen}\lambda}. \, {\color{blue}\#1}$ \footnote{We will sometimes use colors to make it easier to see which index refers to which binder.}.

An index greater than the number of binders that it is surrounded by refers to free variables. We can list the free variables in form of an environment next to the term, so that it is clearer what those indices refer to. In the example below, we compare a term using names and the same term with De Bruijn indices in which variables are colored accordingly to which binder (or free variable) they refer to:
\begin{align*}
 {\color{OliveGreen}\tpd{f}{T_1}}; \; {\color{brown}\tpd{g}{T_2}} \vdash & {\color{blue} \lambda x}. \; {\color{Plum}\lambda y}. \; {\color{OliveGreen}f} \; {\color{blue}x} \; {\color{Plum}y} \\
  {\color{OliveGreen} T_1}; \; {\color{brown}T_2} \vdash & {\color{blue} \lambda}. \;\; {\color{Plum}\lambda}. \;\; {\color{OliveGreen} \#3} \; {\color{blue}\#1} \; {\color{Plum} \#0}
\end{align*}

It is important to note, that the same index can point to different variables and the same variable can be pointed to by different indices --- the index depends heavily on the context (as the \textit{binder distance} is different in different contexts), as shown in the example below:
\begin{align*}
{\color{OliveGreen}\tpd{f}{T_1}} \vdash & {\color{blue} \lambda x}. \; ({\color{Plum}\lambda y}. \; {\color{OliveGreen}f} \; {\color{blue}x} \; {\color{Plum}y}) \; {\color{blue}x} \\
{\color{OliveGreen}T_1} \vdash & {\color{blue} \lambda}. \;\; ({\color{Plum}\lambda}. \; {\color{OliveGreen}\#2} \; {\color{blue}\#1} \; {\color{Plum}\#0}) \; {\color{blue}\#0}
\end{align*}

During substitution, it is important to keep track of the index. When traversing inside of a binder, free variables in the substitute term have to be updated to still refer to the same variable.
For example, given the term $(\lambda. \, \lambda. \, \#1) \; (\lambda. \, {\color{blue}\#2})$, where the index ${\color{blue}\#2}$ refers to the second free variable, after $\beta$-reduction, we get $\lambda. \lambda. \, {\color{blue}\#3}$ --- the substitute term has to be shifted to still refer to the second free variable (as it is now behind not one but two binders).


\subsubsection{\textit{DbLib}}

As handling variable binding is common to nearly all calculi formalizations, there are some Coq libraries that help automate parts of reasoning about them. In this project we are using the \textit{DbLib} library \cite{dblib}, some other choices are discussed in section \ref{binderlibs}.
\label{traverse}
When using \textit{DbLib} we freely define our syntactic forms as some inductive type. We have two mutually inductive types - typed terms (\texttt{typedterm}) and untyped terms (\texttt{term}), and we can substitute an untyped term into both of these types. To use the automation facilities provided by the library, we need to define:
\begin{itemize}
  \item an instance of the typeclass \texttt{Var} which is used for constructing variables - it requires a function \texttt{var} that takes a De Bruijn index (which is simply a natural number) and returns an instance of the type that we will be substituting (in our case \texttt{term}); for us the instance is just calling the constructor \texttt{var x = VAR x}\footnote{The constructor is capitalized (\texttt{VAR}) instead of just \texttt{Var} to avoid name clash with the mentioned typeclass.}
  \item instances of the typeclass \texttt{Traverse} which requires a function \texttt{traverse} taking 3 parameters: a function \texttt{f} representing the operation that is applied to the term-tree, a natural number \texttt{l} representing the current level and a term \texttt{t} that is traversed. It has to recursively traverse all subterms in \texttt{t}, increasing \texttt{l} when entering a binder, and each encountered instance of a variable \texttt{VAR x} has to be replaced with \texttt{f l x}.
\end{itemize}
Afterwards some lemmas have to be proven to show that the \texttt{traverse} function satisfies the required constraints. For example it has to be injective. If the function is not overly complex, these properties are derived automatically by tactics provided by the library.

Once this is defined, substitution, shift etc.\@ are automatically derived from the \texttt{traverse} function.

\subsection{Patterns}

Defining patterns is particularly complicated as the patterns can be arbitrarily nested, so a pattern match could introduce an arbitrary amount of bindings. This is possible to handle, but is much harder to mechanize. To simplify the mechanization, we forbid nested patterns. This does not limit the expressivity of the calculus as any nested pattern match can be converted to a series of non-nested patterns.


For example, we can convert 
\begin{equation*}
\patmat{e}{(\patapp{(\patlam{f}{Nat \to Nat})}{42})}{\app{f}{\square(10)}}{e_{fail}}
\end{equation*}
into
\begin{align*}
e \sim & (\texttt{MatchApp} \, [Nat \to Nat] \, a_1 \, [Nat] \, a_2) \; ? \\
&\Bigg(a_1 \sim (\texttt{MatchLam}[Nat \to Nat] \, f) \; ? \; \big(a_2 \sim \texttt{MatchNat} \, 42 \; ? \; (f \; \square(10))\; || \; e_{fail}\big)\; || \; e_{fail}\Bigg)\; || \; e_{fail}
\end{align*}
(where \texttt{MatchApp} pattern is a special pattern which matches an application and binds the first and second term to $a_1$ and $a_2$ respectively).

%After simplifying to non-nested patterns, the \texttt{bind} pattern is not useful anymore, so we remove it (in non-nested approach it would be equivalent to a let statement). However all of the other patterns that can bind something are changed in a way that they emulate containing the \texttt{bind}, as shown in the example above. Ultimately, we have the following patterns: Nat, Var, Fix, App, Unlift, Lam.

Even after the simplification, the App pattern still introduces two variable bindings, so we need to be able to support it. This is still much simpler than the arbitrary amount of bindings that we started with.

We first tried to define the patterns as a separate inductive definition and have a single syntactic form for the pattern matching:
\begin{verbatim}
Inductive pattrn :=
| PNat (n : nat)
| PVar (x : DeBruijnIndex)
| PBindUnlift
| PBindApp (T1 T2 : type)
| PBindLam (T : type)
| PBindFix.

Inductive typedterm :=
| TypedTerm (u: term) (t: type)
with term :=
| Nat (n : nat)
| Var (x : DeBruijnIndex)
| Lam (argT: type) (ebody : typedterm)
| App (e1 e2 : typedterm)
| Fix (e : typedterm)
| Lift (e : typedterm)
| PatternMatch (e : typedterm) (pat : pattrn) (success failure : typedterm).
\end{verbatim}

Unfortunately this gives rise to 3 syntactic forms (untyped terms, typed terms and patterns) and it seemed to be too much for \textit{DbLib} --- the automatic facilities that help us prove the basic lemmas to make the library usable failed and I was also unable to fix the proofs manually.

So in the end we created a separate syntactic form for each pattern. This may look strange, but it is in fact equivalent to the previous formulation, the difference is only technical / notational.

\begin{verbatim}
Inductive typedterm :=
| TypedTerm (u: term) (t: type)
with term :=
| Nat (n : nat)
| VAR (x : DeBruijnIndex)
| Lam (argT: type) (ebody : typedterm)
| App (e1 e2 : typedterm)
| Fix (e : typedterm)
| Lift (e: typedterm)
| Quote (e : typedterm)
| Splice (e : typedterm)
| MatchNat (e : typedterm) (n : nat) (es ef : typedterm)
| MatchVar (e : typedterm) (x : term) (es ef : typedterm)
| MatchApp (e : typedterm) (T1 T2 : type) (es ef : typedterm)
| MatchUnlift (e : typedterm) (es ef : typedterm)
| MatchLam (e : typedterm) (T : type) (es ef : typedterm)
| MatchFix (e : typedterm) (T : type) (es ef : typedterm).
\end{verbatim}

One other modification that we have to apply here is to the variable matching pattern. This pattern checks if the boxed code is a reference to some variable, so after the De Bruijn translation, this pattern also contains a De Bruijn index for the variable that is matched (see \texttt{PVar} in the first listing). This occurrence is unusual, because we need to update it when shifting all indices in a term, so that it always refers to the original variable (why we have to shift is explained in the previous section). It has to be treated differently, because it should never be substituted with some concrete value.

However in the library that we use, the \texttt{traverse} function has type \texttt{(nat $\to$ nat $\to$ term) $\to$ (nat $\to$ T $\to$ T)} (where \texttt{T} is either a \texttt{term} or \texttt{typedterm}). \texttt{traverse f l t} traverses the term \texttt{t}, incrementing \texttt{l} when entering binders and replacing a variable with index \texttt{x} with the result of \texttt{f l x}. As seen in the type signature, result of \texttt{f l x} can be any \texttt{term}. Because of that, we cannot at this point prove that the result of shifting a variable inside a pattern will remain a variable.

Instead, we replace the raw De Bruijn index in \texttt{MatchVar} with an arbitrary term. This satisfies all the constraints required by the library and ensures that this index is updated when necessary. Unfortunately this also allows for terms that are ill-formed syntactically, (variable match over something other than a variable). This issue is however addressed in the typing rules --- only instances of \texttt{MatchVar} in which \texttt{x} has the form \texttt{VAR y} are accepted. Thus, when we prove Preservation, we also prove that this variable is never substituted for (as the term resulting from such substitution would be ill-typed).

\subsection{Semantics and Typing}
\label{finalformencoding}

The typing judgement is defined as an inductive type \texttt{TypEnv $\to$ Level $\to$ typedterm $\to$ type $\to$ Prop} and the small-step semantics relation is defined as \texttt{Level $\to$ typedterm $\to$ typedterm $\to$ Prop}. 

We use Coq's Notations to make our judgements more readable. The typing judgement is expressed as \texttt{G $\vdash$(L) e $\in$ T} and the semantics relation as \texttt{e1 -->(L) e2} which quite closely resembles the notation used in the paper.

We also define a relation \texttt{evaluates} which is the reflexive transitive closure of small-step semantics at level 0 that is used in tests. For example we can define a term representing applying the identity function to a number and prove that it typechecks and evaluates correctly:
\begin{lstlisting}[mathescape=true]
Definition id_nat := (Lam TNat (VAR 0 : TNat) : TNat ==> TNat).
Definition id_applied := (App id_nat (Nat 42 : TNat) : TNat).
Lemma id_app_types : $\emptyset$ $\vdash$(L0) id_applied $\in$ TNat.
  repeat econstructor.
Qed.
Lemma id_app_evals : evaluates id_applied (Nat 42 : TNat).
  eapply star_step. repeat econstructor.
  unfold substitute. simpl_subst_all. auto.
Qed.
\end{lstlisting}

Another important change from the paper version is the typing environment. On paper it is defined as a (partial) mapping from variable names to types and levels.
\textit{DbLib} is also providing support for using environments with De Bruijn indices. We used the approach suggested by the library authors --- our environment is represented by a partial mapping from natural numbers to types. It is defined as a list of options. This integrates very well with operations on De Bruijn indices. For example, when the environment is extended while going inside a binder, the new type is just prepended to the list. This both handles the new type and the fact that other indices are shifted.

Our definitions always extend the environment without leaving holes, so we could have used lists instead lists of options. But we choose the latter, because the \textit{DbLib} library provides some helpful tactics for handling environments defined that way. This also simplifies the definition of the \textit{Weakening} and \textit{Substitution} lemmas, as we can use an operation that inserts a variable to an environment with an arbitrary index, possibly leaving holes.

As our typing environment has to keep track of level of each variable, the final \texttt{TypEnv} type is \texttt{list (option (level * type))}. The judgement from the original paper $\Gamma(x^i) = T$ is expressed as \texttt{lookup $x$ $\Gamma$ = Some (L$i$, $T$)}.

Extending the environment, i.e. $\Gamma; x^i : T$ is expressed as \verb|insert x (Li, T) |$\Gamma$, where the new \verb|x| is the De Bruijn index corresponding to $x$. In the typing rules we only extend the environment by introducing a new binder which is the closest binder now (i.e. its index will be 0) and all the other binder's indices are shifted by 1. This operation is expressed by \verb|insert 0 (Li, T) |$\Gamma$.

So for example, the abstraction rule
\begin{framed}
\infrule{ \Gamma,x^i{:}T_1 \vdash^i t_2 \in T_2 }{\Gamma \vdash^i \tpd{(\lam{x}{T_1}{t_2})}{T_1{\to}T_2} \; \in \; T_1 \to T_2}
\end{framed}
is expressed as
\begin{lstlisting}[mathescape=true]
| T_Abs : forall Li G T1 T2 t2,
    insert 0 (Li, T1) G $\vdash$(Li) t2 $\in$ T2 ->
    G $\vdash$(Li) (Lam T1 t2 : T1 ==> T2) $\in$ (T1 ==> T2)
\end{lstlisting}



\section{Proving Soundness}
\label{sec:proving}
We want to prove
\begin{theorem}[Progress]
 If $\emptyset \vdash^0 t \in T$, then $t$ is a value or there exists $t'$ such $t \longrightarrow^0 t'$ 
\end{theorem}
and
\begin{theorem}[Preservation]
If $\Gamma \vdash^i t \in T$ and $t \longrightarrow^i t'$, then $\Gamma \vdash^i t' \in T$.
\end{theorem}

First we had to reformulate the theorems into our language. Thanks to Coq's notations they don't differ much:
\begin{lstlisting}[mathescape=true]
Theorem Progress : forall t T,
  $\emptyset$ $\vdash$(L0) t $\in$ T ->
  isvalue t \/ exists t', t -->(L0) t'.

Theorem Preservation : forall t1 T G L,
  G $\vdash$(L) t1 $\in$ T ->
  forall t2,
  t1 -->(L) t2 ->
  G $\vdash$(L) t2 $\in$ T.
\end{lstlisting}

\subsection{Induction}

As we have two mutually inductive syntactic categories (the untyped terms and typed terms that wrap the untyped ones), we need an induction principle that handles them well. The default principle generated by Coq is not convenient to use as it requires separate properties for typed terms and untyped terms. However in proofs, we usually want to only prove the property on typed terms. We created a custom induction principle that only requires a property on typed terms, because the analogous property on untyped terms is inferred. The idea is further discussed in section \ref{sec:induction}.

\subsection{Progress}

The progress theorem in the original proof relies mostly on an auxiliary lemma - \textit{Level Progress}. It is defined in two parts, namely:
\begin{lemma}[Level Progress]
For any given term $t$, we have:
\begin{itemize}
  \item[(1)]  If $\Gamma^{[1]} \vdash^0 t \in T$, then $t$ is a value or  there exists $t'$ such that $t \longrightarrow^0 t'$.
  \item[(2)] If $\Gamma^{[1]} \vdash^1 t \in T$ and $(\square t) : \square T$ is not a value,  then there exists $t'$ such that $t \longrightarrow^1 t'$.
\end{itemize}
\end{lemma}
Where $\Gamma^{[1]}$ means that the environment only contains level 1 variables.

Unsurprisingly, when proving by induction, each case needs the other. One approach to formulating this in Coq would be to define mutually recursive lemmas using the \verb|Lemma ... with| syntax. This is however quite prone to mistakes, because the fixpoint check is only done at the end, so it is easy to create a self-referencing proof that will be rejected only after typing \texttt{Qed} at its end.

Instead we formulate this as a single lemma with a disjunction:
\begin{lstlisting}[mathescape=true]
Lemma LevelProgress : forall t G T L,
  RestrictedLevel G L1 ->
  G $\vdash$(L) t $\in$ T ->
     (L = L0 /\ (isvalue t \/ exists t', t -->(L0) t')) 
  \/ (L = L1 /\ (not (isvalue (Quote t : $\square$T)) -> exists t', t -->(L1) t')).
\end{lstlisting}

This allows us to prove both cases based on syntactic induction on $t$. Thanks to the induction principle defined earlier, we have all the necessary assumptions.

In the proof, in each case we \verb|destruct L| getting two cases where \verb|L| has a concrete value. Therefore, in the inductive hypothesis conclusion only one of the alternatives is true. We proved two helper lemmas that can be used simplify the inductive hypothesis, detecting the possible alternative and eliminating the impossible one.

The \textit{Level Progress} proof is quite long as, beside some trivial cases, each syntactic form required a slightly different treatment. However after proving that lemma, the \textit{Progress} theorem is a very simple corollary.

\subsection{Preservation}

To prove Preservation we need two auxiliary lemmas - \textit{Weakening} and \textit{Substitution}. These are closely related to handling variables in the environment, so they have to be quite deeply modified when translating to De Bruijn indices.

\textit{Weakening} is originally defined as 
\begin{lemma}[Weakening]
If $\Gamma^{[1]} \vdash^i t \in T, x \not\in FV(\Gamma)$, then $\Gamma, x^j : T \vdash^i t \in T$.
\end{lemma}

The modification is rather intuitive, as explained in section \ref{finalformencoding}, extending the environment is defined using \verb|insert| and instead of a variable name $x$ we have an index \verb|x|. The fact that $x \not\in FV(\Gamma)$ is expressed by showing the property for \verb|shift x t| which shifts the indices inside of \verb|t| so that the \verb|x|th variable is not bound in that term, and other indices are modified in the same way as they would have been shifted by \verb|insert|, so that they still correspond to the original binders. It is defined as:
\begin{lstlisting}[mathescape=true]
Lemma Weakening: forall G L t T,
  G $\vdash$(L) t $\in$ T ->
  forall x L' T' G',
  insert x (L', T') G = G' ->
  G' $\vdash$(L) (shift x t) $\in$ T.
\end{lstlisting}

\paragraph{}

The \textit{Substitution} lemma is a bit more complicated than usual:
\begin{lemma}[Substitution]
If (1) $\Gamma \vdash^j t_1 \in T_1$, (2) $\Gamma, x^j : T_1 \vdash^i t_2 \in T_2$ and
(3) $j = 0$ or $t_2$ does not contain pattern matches, then $\Gamma \vdash^i t_2[x \mapsto t_1] \in T_2$.
\end{lemma}

The first two conditions are usual, the third one is exceptional.
The third condition says that we can substitute inside arbitrary terms for variables at level 0, but when substituting a variable at level 1, the term that we are substituting into cannot contain any pattern matches. Without this assumption, we may arrive at a situation when we are substituting a variable inside a pattern matching a particular variable (ie. $(t \sim x \; ? \; t_a \; || \; t_b)[x \mapsto t_c]$), we would get a result that is not even syntactically correct nor typable. So we just disallow this kinds of situations (note that the variable pattern only applies to level 1 variables). 

At first it may seem like a too big restriction, making the lemma too weak. But in fact this is enough, because we are mostly doing substitutions at level 0. The only situation when we are substituting a level 1 variable is when lifting a lambda in the $\patlam{e}{T}$ pattern\footnote{See figure \ref{fig:pattern-semantics} in the Appendix for details.}. But in this particular case, we know that the whole lambda term is plain, so the $e$ inside it does not contain the pattern match, as required.

Once we understand the nuance about the third condition, translating the lemma into Coq is quite simple, so we skip it.

\paragraph{}

The Preservation proof also uses a few more auxiliary lemmas to simplify some properties. For example we prove that a plain term doesn't contain pattern matches (which is almost by definition) or that renaming (ie. shifting indices in the De Bruijn formulation) doesn't introduce pattern matches if they weren't present originally. % As in the language's rules we also define a simplified version of substitution lemma that is closer to the notation used in the definitions and is a direct corollary of the main Substitution lemma.

We extensively use the tactics provided by \textit{DbLib} for dealing with substitutions and define some more tactics specific to our formulation that help simplifying substitutions in the proof context.
\section{Discussion}
\label{sec:discussion}
\subsection{Multiple binders at once}
\label{multibind}

Patterns that represent multiple binders and are substituted at once have to be considered very carefully as it is non-trivial to judge which indices to shift and how.

First, let's see more closely how beta-reduction works with De Bruijn indices.

To show how the indices behave, we assume there are some free variables that are not reduced (these are put on the left side of $\vdash$ indicating the environment). In our language this happens because we have variables at two levels, and while level 0 variables are reduced, level 1 are not.

For clarity, to show the intermediate step, we define the substitution operation $(e_1)[e_2/]$ that substitutes all De Bruijn indices equal to 0 in $e_1$ with $e_2$ and decreases other indices (to indicate that after substitution we have got rid of one binder). As the substitution operation is an intermediate step, the enclosing lambda is removed already, but the indices are not yet shifted, so to keep the De Bruijn indices consistent, we treat the parentheses around $(e_1)$ as a binder. In $(e_1)[e_2]$, $e_1$ is still inside a binder that will replace its first free variable with $e_2$. With these definitions, beta-reduction will proceed as follows:
\begin{align*}
{\color{blue}T_2}; {\color{OliveGreen}T_1} &\vdash ({\color{Fuchsia}\lambda}. \; {\color{Fuchsia}\#0} \; {\color{OliveGreen}\#1}) \; {\color{blue}\#1} \\
{\color{blue}T_2}; {\color{OliveGreen}T_1} &\vdash {\color{Fuchsia}({\color{Fuchsia}\#0} \; {\color{OliveGreen}\#1})}[{\color{blue}\#1}/] \\
{\color{blue}T_2}; {\color{OliveGreen}T_1} &\vdash {\color{blue}\#1} \; {\color{OliveGreen}\#0}
\end{align*}

Now, let's analyse how we can define matching a 2-element tuple (in our case we were matching an application, but we have chosen this example as it is more familiar). Let's define $\texttt{unpack } e \texttt{ as } (x_1, x_2) \texttt{ in } t$ as the operation that deconstructs a tuple $e = (e_1, e_2)$ and binds $e_1$ to $x_1$ and $e_2$ to $x_2$ in $t$.

As we are in the realm of De Bruijn indices, we modify that syntax to $\texttt{unpack } e \texttt{ as } (\bullet, \bullet) \texttt{ in } t$ where the two bullets count as two binders. Inside of $t$, the closest De Bruijn index (0) is bound to $e_2$ and the next one (1) to $e_1$ (we use the order that may seem reversed but that is to express that $e_2$ corresponds to the second bullet which is closer to $t$ than the one that $e_1$ corresponds to which is consistent with the indices representing the \textit{binder distance}). To illustrate, $\texttt{unpack } (e_1, e_2) \texttt{ as } ({\color{blue}\bullet}, {\color{OliveGreen}\bullet}) \texttt{ in } ({\color{blue}\#1} \; {\color{OliveGreen}\#0}) \longrightarrow (e_1 \; e_2)$.

It is tempting to define the reduction rule as $\texttt{unpack } (e_1, e_2) \texttt{ as } (\bullet, \bullet) \texttt{ in } t \longrightarrow ((t)[e_2/])[e_1/]$. But let's analyse how that would work on an example with free variables:
\begin{align*}
{\color{blue}T_2}; {\color{OliveGreen}T_1} &\vdash \texttt{unpack } ({\color{blue}\#1}, {\color{OliveGreen}\#0}) \texttt{ as } ({\color{Fuchsia}\bullet}, {\color{BurntOrange}\bullet}) \texttt{ in } ({\color{Fuchsia}\#1} \; {\color{BurntOrange}\#0}) \\
{\color{blue}T_2}; {\color{OliveGreen}T_1} &\vdash {\color{Fuchsia}(}{\color{BurntOrange}({\color{Fuchsia}\#1} \; {\color{BurntOrange}\#0})}[{\color{red}\#0}/]{\color{Fuchsia})}[{\color{blue}\#1}/] \\
{\color{blue}T_2}; {\color{OliveGreen}T_1} &\vdash {\color{Fuchsia}(}{\color{Fuchsia}\#0} \; {\color{red}\#0}{\color{Fuchsia})}[{\color{blue}\#1}/] \\
{\color{blue}T_2}; {\color{OliveGreen}T_1} &\vdash ({\color{blue}\#1} \; {\color{blue}\#1})
\end{align*}

We would expect to get $({\color{blue}\#1} \; {\color{OliveGreen}\#0})$ as the result, but instead we get $({\color{blue}\#1} \; {\color{blue}\#1})$. We can see that something is wrong in the second line already. The ${\color{OliveGreen}\#0}$ that will be substituted, is kept as-is, as index 0. But that variable went inside the scope of the ${\color{Fuchsia}\bullet}$ binder (represented in the substitution operation as the parentheses ${\color{Fuchsia}( \; )}$). So in that context, the index $\#0$ actually points to the ${\color{Fuchsia}( \; )}$ binder and not ${\color{OliveGreen}T_1}$ as it should have (that's why we marked it {\color{red}red}, because it actually refers to something else than what we wanted). The issue becomes completely clear after performing the first substitution - both variables inside are now $\#0$ so they necessarily refer to the same thing, even though initially they were not meant to.

The issue is that, when performing substitutions one-by-one, the substituted expression $e_2$ goes inside the scope of the other binder corresponding to $e_1$, so references in $e_2$ can be mistakenly captured to refer to $e_1$.

To deal with this, we need to make sure that the indices inside of $e_2$ are updated correctly, so that the binder corresponding to $e_1$ is out of its scope. That is exactly what the \texttt{shift} function is for. \texttt{shift} simply increases all indices in a term by one, allowing us to go inside a scope of a binder, but without binding to it (so that all variables still reference the same things from the outside).

The updated reduction rule will be $\texttt{unpack } (e_1, e_2) \texttt{ as } (\bullet, \bullet) \texttt{ in } t \longrightarrow ((t)[\texttt{shift }e_2/])[e_1/]$ where $\texttt{shift }e_2$ ensures that all variables in $e_2$ still point to the same things, even though we moved $e_2$ inside a temporary.

We can revisit our example:
\begin{align*}
{\color{blue}T_2}; {\color{OliveGreen}T_1} &\vdash \texttt{unpack } ({\color{blue}\#1}, {\color{OliveGreen}\#0}) \texttt{ as } ({\color{Fuchsia}\bullet}, {\color{BurntOrange}\bullet}) \texttt{ in } ({\color{Fuchsia}\#1} \; {\color{BurntOrange}\#0}) \\
{\color{blue}T_2}; {\color{OliveGreen}T_1} &\vdash {\color{Fuchsia}(}{\color{BurntOrange}({\color{Fuchsia}\#1} \; {\color{BurntOrange}\#0})}[{(\texttt{shift }\#0)}/]{\color{Fuchsia})}[{\color{blue}\#1}/] \\
{\color{blue}T_2}; {\color{OliveGreen}T_1} &\vdash {\color{Fuchsia}(}{\color{BurntOrange}({\color{Fuchsia}\#1} \; {\color{BurntOrange}\#0})}[{\color{OliveGreen}\#1}/]{\color{Fuchsia})}[{\color{blue}\#1}/] \\
{\color{blue}T_2}; {\color{OliveGreen}T_1} &\vdash {\color{Fuchsia}(}{\color{Fuchsia}\#0} \; {\color{OliveGreen}\#1}{\color{Fuchsia})}[{\color{blue}\#1}/] \\
{\color{blue}T_2}; {\color{OliveGreen}T_1} &\vdash ({\color{blue}\#1} \; {\color{OliveGreen}\#0})
\end{align*}

To sum up, when doing multiple substitutions one-by-one, we need to keep in mind that terms that have been substituted first are inside the scope of all further substitutions and can capture variables from them (and get their indices mixed-up completely!). To avoid this problem we need to \texttt{shift} the terms to preserve consistency of the references. When substituting more than 2 binders at once, we may even need to \texttt{shift} some terms multiple times.

\subsection{Induction principle for related types}
In our language we have typed and untyped terms that are mutually inductive. We often want to prove some properties about them using syntactic induction. However the induction principles generated by default may not always be sufficient.

To illustrate the concept we will use a simplified definition:
\begin{lstlisting}
Inductive Typed :=
| typed (u: Untyped) (T: Type)
with
Untyped :=
| leaf
| branch (t1 t2 : Typed).
\end{lstlisting}

The default induction principle generated by Coq for \texttt{Typed} is:
\begin{lstlisting}
forall P : Typed -> Prop,
  (forall (u : Untyped) (T : Type), P (typed u T)) -> 
  forall t : Typed, P t
\end{lstlisting}

When proving a theorem about typed terms, we need to show that some property holds for all untyped terms, but we don't get any inductive hypotheses about them in this induction principle. For most theorems such principle is too weak.

To improve on that, we can use Coq's \texttt{Scheme} to generate mutually inductive principles.

\begin{lstlisting}
Scheme Typed_mutualind := Induction for Typed Sort Prop
  with Untyped_mutualind := Induction for Untyped Sort Prop.
\end{lstlisting}

This gives us the following principle for typed terms:
\begin{lstlisting}
forall (P : Typed -> Prop) (P0 : Untyped -> Prop),
  (forall u : Untyped, P0 u -> forall T : Type, P (typed u T)) ->
  P0 leaf ->
  (forall t1 : Typed, P t1 -> forall t2 : Typed, P t2 -> P0 (branch t1 t2)) ->
  forall t : Typed, P t
\end{lstlisting}

This is much better. The typed terms are now destructed and we have to prove cases for each of the two syntactic forms. Moreover, for \textit{branch} which contains typed terms as elements, we have the inductive hypothesis that the property holds for both of its elements.

This induction principle is already good enough, because it gives us all the necessary assumptions. In this principle, we have to properties that we prove and an assumption that relates them. The downside is that when using this induction principle, we usually have a goal with one property and the other one has to be defined every time. In the setting of the dual typed and untyped terms, we can do better.

We can define \texttt{P0} such that \texttt{P0 u -> forall T : Type, P (typed u T)}. We then get:

\begin{lstlisting}
Lemma Typed_syntactic :
  forall (P : Typed -> Prop),
    (forall T, P (typed leaf T)) ->
    (forall (t1 t2 : Typed) T, P t1 ->  P t2 -> P (typed (branch t1 t2) T)) ->
    forall t : Typed, P t.
\end{lstlisting}

We wrap all of the untyped variants into typed variants with an arbitrary type. This allows us to reuse the property defined for typed terms. This is very convenient, as now we can just do \texttt{induction term using Typed\_syntactic} and we will get goals for all the untyped cases.

\subsection{Variable binding libraries}
\label{binderlibs}

As handling variable binding is a common problem for all programming language formalizations, there are some libraries that help dealing with it. Below we describe three libraries that we approached and our experiences using them.

\paragraph{}

\textit{autosubst} \cite{autosubst} is quite simple to use. We use special types when defining the syntactic forms: a separate type \verb|var| for the De Bruijn index which is an alias for \verb|nat| but it is tagged so that the library knows this is an index and handles it properly; and a type constructor \verb|bind| that wraps any type, indicating that anything inside it consumes one binder. All properties are derived automatically. A big advantage of this library is that its simplification tactics are quite powerful, at least in our experience they worked out of the box most of the time. It is a very user-friendly and robust solution for simple calculi.

Unfortunately, \textit{autosubst} was too simple for our use-case. In theory it supports having two types of syntax forms, but we had issues with using it. It also only allows binding one variable per syntactic form whereas we need to bind at least two (for \verb|MatchApp|). It may be possible to work around this issue by introducing some dummy syntactic forms (but before we thought about this possible trick we already switched to the more flexible \textit{DbLib}).

% It could be possible to work around this issue by introducing an intermediate syntactic form so that \verb|MatchApp| is divided into two forms, ie \verb|MatchApp e T1 T2 es ef| could become \verb|MatchApp1 e T1 T2 (MatchApp2 es) ef| (because \verb|es| is the term that needs to bind two variables). We didn't test if this would work, because before coming up with it we tried a different library, but it would be a rather 'hacky' approach.

\paragraph{}

We chose to use \textit{DbLib} \cite{dblib}. It is a bit harder to start with but is much more flexible. Instead of auto-generating how the binders behave (like \textit{autosubst} did), the user defines the syntactic forms and a \verb|traverse| function which describes how the binders behave (the \verb|traverse| function is described in section \ref{traverse}). This allows for having multiple bindings. The library also supports multiple syntactic categories, although when using that feature it may be necessary to manually call the tactics that automate derivation of basic properties. For more complex syntactic structures, these tactics failed, so we had to keep the structure quite simple. The tactics for simplifying substitution were a bit less robust than in \textit{autosubst} and sometimes needed some help from the user, but it seems reasonable given that this library allows for more complicated syntax definitions.

\paragraph{}

Another approach worth mentioning is \textit{Locally Nameless} \cite{locallynameless}. It is more of an approach than a library (although the author shares a library with useful tactics). It is a hybrid approach that uses De Bruijn indices for bound variables but uses some arbitrary naming (as one would in a normal programming language) for the free variables (so the syntax needs two forms for variables - one for bound variables and another one for free variables). Using De Bruijn indices for bound variables makes it easier to manually define the substitution operation (whereas in the other libraries it was derived automatically) which is split into two operations - there is one function for substituting free variables and separate one for substituting the first DeBruijn index (which is called \textit{opening} the term). 

It seems to be the most flexible approach but also the most complex one, as the user has to do most of the work and the amount of automation provided by the library is rather limited. It is also based on quite complicated principles (De Bruijn indices are hard to understand on their own and mixing them with normal names makes it even more difficult). It is a good choice for complex calculi for which the other libraries would be too limited.

\paragraph{}

In this project, the choice of the library was quite highly influenced by my level of understanding. I started with \textit{autosubst} which was the simplest one and quite easy to learn, but then it seemed that it imposed some constraints that didn't allow to express important parts of the calculus (although it may have been possible to work around them with some clever but ugly tricks). Then I tried both \textit{Locally Nameless} and \textit{DbLib}, but at that point \textit{Locally Nameless} was still too hard to understand it to be able to use it. \textit{DbLib} was a nice middle-ground between flexibility and complexity and it still offered decent automation. After using it and understanding better more nuances of using De Bruijn indices, I can appreciate the beauty of \textit{Locally Nameless} approach, but given its limited amount of automation, it is a library best used for more complex calculi.

\subsection{Testing before proving}
Adding the pattern matching was a very delicate matter, especially as we had to diverge slightly from the original calculus (by defining the separate syntactic categories for all pattern types). It was easy to make mistakes in the definitions of types and syntax (for example one of the bugs was having the two variables bound by app in different order in the typing judgement and in the semantics). Such mistakes lead to unprovable theorems, but they may not be easy to notice early during proving. To avoid such issues, I decided to write some \emph{unit tests} - I defined simple programs in the calculus that I know should work and tried proving that they do type-check and evaluate correctly. These proofs were usually rather easy but they helped find lots of simple mistakes. Moreover, they can serve as examples of how to use the calculus in the formalized version which helps understanding.

It seems like a very good approach to write some simple tests (for example lemmas about the desired properties of some specific simple terms, like the ones at the end of section \ref{finalformencoding}) before moving on to the general soundness proofs.

\subsection{Iterative development}
We have approached the proofs by first trying to prove a subset of the calculus corresponding to the \textit{simply typed lambda calculus}, as a warm-up. Then we have been adding more features one by one and updating the proofs. While such approach sometimes requires rewriting substantial parts of the proofs, it allows to keep the scope more manageable. Moreover, it allows for partial results along the way, so it is a kind of a safety net. Even if some later stage were to turn out too problematic to do, the iterative approach ensures that there are partial outcomes that may stand on their own.

\subsection{Proof stability, naming}

Another good practice that I learnt along the way is trying to make the proofs resistant to changes. It is quite common to have to add some additional assumption in one of the definitions or simply extend the calculus with new definitions. If we don't write our proofs carefully, a small modification of the definitions may require us to rewrite most of them, while writing them well may result in not having to change anything.

Firstly, it's good to use as much automation as possible, as it is usually robust to small changes. When introducing hypotheses (with \verb|intro| or \verb|assert|) it is good to give them names, because the automatic naming may very easily change if an additional assumption is added in the future.

In big case-by-case proofs (for example syntactic induction), I usually try to come up with some basic strategy that solves at least the simple goals (using \verb|induction X; try solve [general strategy].|), but in more complex proofs we may still have to manually approach many goals. It is then a good idea when focusing a new goal to write a comment shortly describing the current goal. This helps when by adding some additional rules, the order of the goal changes (result in applying tactics to wrong goals and very confusing errors). These comments help to quickly catch situations when the current goal is different than would be expected.

I didn't find a way to name hypotheses introduced by calling the \verb|inversion| tactics (which I use extensively), so I still had to deal with unpredictable hypothesis names. It is also problematic when combining tactics. For example in the chain \verb|induction t; inversion H1.|, \verb|H1| may introduce different sets of hypotheses for each case. Sometimes I also want to call \verb|inversion| but the particular hypothesis that I want to examine will be called differently in different branches of a tactic chain similar to the one just mentioned. The solution to these issues is matching the goal - the ability to write tactics that match the current goal or available hypotheses against some patterns. For example we can write:
\begin{lstlisting}[mathescape=true]
Ltac invV :=
  match goal with
  | H: ?G $\vdash$(L0) ?v $\in$ $\square$(?T) |- _ => inversion H; subst
  end.
\end{lstlisting}
which defines a tactic that finds a hypothesis containing a typing judgement for some quoted type (a $\square T$ for some $T$) and runs \verb|inversion| on this hypothesis. Now we can write \verb|induction t; invV.| and regardless of what that hypothesis would be called, we can find it and inside of \verb|invV| bind its name to \verb|H|. This is a very powerful technique that allows to write proofs robust to changes.


\section{Summary}
\label{sec:summary}

We have described the \calculus calculus and various challenges in formalizing it in Coq and proving its soundness. We discuss some interesting problems that had to be solved along the way. Our main contribution is the mechanized formalization of the calculus along with its soundness proofs that can be found at \href{https://github.com/radeusgd/QuotedPatternMatchingProof/}{https://github.com/radeusgd/QuotedPatternMatchingProof/}.

\bibliographystyle{unsrt}
\bibliography{bibliography}

\section*{Appendix}



To simplify the proofs, we remove nested patterns leaving only simpler non-nested pattern matching. This requires changing some typing and semantic rules.

For clarity, we show how the syntax of our patterns corresponds to non-nested patterns in the original syntax in figure \ref{fig:pattern-syntax}. As we use De Bruijn indices, the definitions in the mechanization look slightly different. So we introduce fake names for variables bound in the pattern matches that correspond to the De Bruijn indices. $b_0$ stands for the introduced variable name that corresponds to the most closely bound De Bruijn index and $b_1$ stands for the next index.

\newcommand{\syntaxeq}[2]{ #1 \quad &\equiv \quad #2 \\ }

\begin{figure}[h]
  \centering
  \begin{framed}
    \begin{align}
     \syntaxeq
     { \texttt{MatchNat} \; t_1 \; n \; t_2 \; t_3 }
     { \patmat{t_1}{\patnat{n}}{t_2}{t_3} }     
    \syntaxeq
     { \texttt{MatchVar} \; t_1 \; x \; t_2 \; t_3 }
     { \patmat{t_1}{\patref{x}}{t_2}{t_3} }
         \syntaxeq
     { \texttt{MatchApp} \; t_1 \; T_1 \; T_2 \; {\color{gray}b_0} \; {\color{gray}b_1} \; t_2 \; t_3 }
     { \patmat{t_1}{\patapp{(\patbind{b_0}{T_1})}{(\patbind{b_1}{T_1 \to T_2})}}{t_2}{t_3} }     
         \syntaxeq
     { \texttt{MatchUnlift} \; t_1 \; {\color{gray}b_0} \; t_2 \; t_3 }
     { \patmat{t_1}{\patunlift{b_0}}{t_2}{t_3} }  
         \syntaxeq
     { \texttt{MatchLam} \; t_1 \; (T_1 \to T_2) \; {\color{gray}b_0} \; t_2 \; t_3 }
     { \patmat{t_1}{\patlam{b_0}{T_1 \to T_2}}{t_2}{t_3} }
     \syntaxeq
     { \texttt{MatchFix} \; (\tpd{t_1}{T_1}) \; {\color{gray}b_0} \; t_2 \; t_3 }
     { \patmat{(\tpd{t_1}{T_1})}{\patfix{(\patbind{b_0}{T_1 \to T_1})}}{t_2}{t_3} }
     \end{align}
  \end{framed}
  \caption{Pattern syntax clarification}
  \label{fig:pattern-syntax}
\end{figure}

As we have separate syntactic forms for each pattern, we remove the general \textsc{T-Pat} rule and its helper pattern typing rules and we replace them with rules outlined in figure \ref{fig:pattern-type-system}. The rules that were kept are listed in \ref{fig:explicitly-typed-type-system}.

\begin{figure}
  \centering
  \begin{framed}
    
    \textbf{Explicitly-typed Terms Typing} \hfill \framebox[1.2\width][r]{$\Gamma \vdash^i t \in T$}
    
    \begin{multicols}{2}
      
      % Coll 1
      
      \vbox{
        \infax[T-Nat]
        { \Gamma \vdash^i \tpd{n}{Nat} \in Nat  }
        \vspace{2mm}
      }
      
      \infrule[T-Var]
      { \Gamma(x^i) = T }
      { \Gamma \vdash^i \tpd{x}{T} \in T  }
      
      \vspace{1mm}
      
      \infrule[T-Abs]
      { \Gamma,x^i{:}T_1 \vdash^i \t_2 \in T_2 }
      { \Gamma \vdash^i \tpd{(\lam{x}{T_1}{\t_2})}{T_1{\to}T_2} \;\in\; T_1{\to}T_2  }
      
      \vspace{1mm}
      
      \infrule[T-App]
      { \Gamma \vdash^i \t_1 \in T_1{\to}T_2 \andalso  \Gamma \vdash^i \t_2 \in T_1}
      { \Gamma \vdash^i \tpd{(\app{\t_1}{\t_2})}{T_2} \;\in\; T_2  }
      
      
      % Coll 2
      
      \infrule[T-Fix]
      { \Gamma \vdash^i \t \in T{\to}T }
      { \Gamma \vdash^i \tpd{(\fix{\t})}{T} \;\in\; T  }
      
      \vspace{1mm}
      
      \infrule[T-Lift]
      { \Gamma \vdash^0 \t \in Nat }
      { \Gamma \vdash^0 \tpd{(\lift{t})}{\qtype{Nat}} \;\in\; \qtype{Nat} }
      
      \vspace{1mm}
      
      \infrule[T-Box]
      { \Gamma \vdash^1 \t \in T }
      { \Gamma \vdash^0 \tpd{(\quoted{\t})}{\qtype{T}} \;\in\; \qtype{T} }
      
      \vspace{1mm}
      
      \infrule[T-Unbox]
      { \Gamma \vdash^0 \t \in \qtype{T} }
      { \Gamma \vdash^1 \tpd{(\splice{\t})}{T} \;\in\; T  }
      
      
    \end{multicols}
    
  \end{framed}
  \caption{Explicitly-typed Terms Typing (retained rules)}
  \label{fig:explicitly-typed-type-system}
\end{figure}

\begin{figure}
\centering
\begin{framed}
    
    \textbf{Explicitly-typed Terms Typing (added rules)} \hfill \framebox[1.2\width][r]{$\Gamma \vdash^i t \in T$}

    \infrule[T-Pat-Nat]
{
  \Gamma \vdash^0 t_1 \in \qtype{Nat} \andalso
  \Gamma \vdash^0 t_2 \in T \andalso
  \Gamma \vdash^0 t_3 \in T
}
{ \Gamma \vdash^0 \tpd{(\patmat{t_1}{\patnat{n}}{t_2}{t_3})}{T} \;\in\; T  }

\vspace{1mm}

\infrule[T-Pat-Var]
{
  \Gamma \vdash^0 t_1 \in \qtype{T_1} \andalso
  \Gamma(x^1) = T_1 \andalso
  \Gamma \vdash^0 t_2 \in T \andalso
  \Gamma \vdash^0 t_3 \in T
}
{ \Gamma \vdash^0 \tpd{(\patmat{t_1}{\patref{x}}{t_2}{t_3})}{T} \;\in\; T  }

\vspace{1mm}

\infrule[T-Pat-App]
{
  \Gamma \vdash^0 t_1 \in \qtype{T_2} \andalso
  \Gamma; \, b_1^0 : \qtype{(T_1 \to T_2)}; \, b_0^0 : \qtype{T_1} \vdash^0 t_2 \in T \andalso
  \Gamma \vdash^0 t_3 \in T
}
{ \Gamma \vdash^0 \tpd{(\patmat{t_1}{\patapp{(\patbind{b_0}{T_1})}{(\patbind{b_1}{T_1 \to T_2})}}{t_2}{t_3})}{T} \;\in\; T  }

\vspace{1mm}

\infrule[T-Pat-Unlift]
{
  \Gamma \vdash^0 t_1 \in \qtype{Nat} \andalso
  \Gamma; \, b_0^0 : Nat \vdash^0 t_2 \in T \andalso
  \Gamma \vdash^0 t_3 \in T
}
{ \Gamma \vdash^0 \tpd{(\patmat{t_1}{\patunlift{b_0}}{t_2}{t_3})}{T} \;\in\; T  }

\vspace{1mm}

\infrule[T-Pat-Lam]
{
  \Gamma \vdash^0 t_1 \in \qtype{T_1 \to T_2} \andalso
  \Gamma; \, b_0^0 : \qtype{T_1} \to \qtype{T_2} \vdash^0 t_2 \in T \andalso
  \Gamma \vdash^0 t_3 \in T
}
{ \Gamma \vdash^0 \tpd{(\patmat{t_1}{\patlam{b_0}{T_1 \to T_2}}{t_2}{t_3})}{T} \;\in\; T  }

\vspace{1mm}

\infrule[T-Pat-Fix]
{
  \Gamma \vdash^0 t_1 \in \qtype{T_1} \andalso
  \Gamma; \, b_0^0 : \qtype{(T_1 \to T_1)} \vdash^0 t_2 \in T \andalso
  \Gamma \vdash^0 t_3 \in T
}
{ \Gamma \vdash^0 \tpd{(\patmat{(\tpd{t_1}{T_1})}{\patfix{(\patbind{b_0}{T_1 \to T_1})}}{t_2}{t_3})}{T} \;\in\; T  }

\end{framed}
\caption{Explicitly-typed Terms Typing (added rules)}
\label{fig:pattern-type-system}
\end{figure}

Similarly, we need separate \textsc{E-Pat-Succ}, \textsc{E-Pat-Fail} and \textsc{E-Pat} for each pattern as they are separate syntactic forms. This however makes the \textit{match} function obsolete, as all matching rules are directly expressed by these separate rules, as listed in figure \ref{fig:pattern-semantics}. The other semantic rules that were kept are listed in figure \ref{fig:operational-semantics}. The notation $\hat\t$ is a shorthand for saying $t \text{ is plain}$.

\begin{figure}
  \centering
  \begin{framed}
    
    \begin{minipage}{0.45\linewidth}
      \infrule[E-App-1]
      { \t_1 \longrightarrow^i \t_1' }
      { \tpd{(\app{\t_1}{\t_2})}{T} \longrightarrow^i \tpd{(\app{\t_1'}{\t_2})}{T} }
      
      \vspace{5mm}
      
      \infrule[E-App-2]
      { \t_2 \longrightarrow^i \t_2' }
      { \tpd{(\app{\t_1}{\t_2})}{T} \longrightarrow^i \tpd{(\app{\t_1}{\t_2'})}{T} }
      
      \vspace{5mm}
      
      \infrule[E-Abs]
      { \t \longrightarrow^1 \t' }
      { \tpd{(\lam{x}{T_1}{\t})}{T} \longrightarrow^1 \tpd{(\lam{x}{T_1}{\t'})}{T} }
      
      \vspace{5mm}
      
      \infax[E-Lift-Red]
      { \tpd{(\lift{n})}{T} \longrightarrow^0 \tpd{(\quoted{n})}{T} }
      
    \end{minipage}
    \begin{minipage}{0.50\linewidth}
      
      \infrule[E-Box]
      { \t \longrightarrow^1 \t' }
      { \tpd{(\quoted{\t})}{T} \longrightarrow^0 \tpd{(\quoted{\t'})}{T} }
      
      \vspace{5mm}
      
      \infrule[E-Unbox]
      { \t \longrightarrow^0 \t' }
      { \tpd{(\splice{\t})}{T} \longrightarrow^1 \tpd{(\splice{\t'})}{T} }
      
      \vspace{8mm}
      
      \infax[E-Splice]
      { \tpd{(\splice{\quoted{\hat\t\,}})}{T} \longrightarrow^1 \tpd{\hat\t}{T} }
      
      \vspace{5mm}
      
      \infrule[E-Lift]
      { \t \longrightarrow^0 \t' }
      { \tpd{(\lift{\t})}{T} \longrightarrow^0 \tpd{(\lift{\t'})}{T} }
      
    \end{minipage}
    
    \vspace{4mm}
    
    \infax[E-Beta]
    { \tpd{(\app{\tpd{(\lam{x}{T_1}{\t})}{(T_1{\to}T_2})}{v})}{T_2} \longrightarrow^0 \t[x \mapsto \strip{v}] }
    
    \vspace{1mm}
    
    \infrule[E-Fix]
    { \t \longrightarrow^i \t' }
    { \tpd{(\fix{\t})}{T} \longrightarrow^i \tpd{(\fix{\t'})}{T} }
    
    \vspace{1mm}
    
    \infax[E-Fix-Red]
    { \tpd{(\fix{\lam{f}{T}{\t})}}{T} \longrightarrow^0 \t[f \mapsto \fix{\lam{f}{T}{\t}}] }
    
  \end{framed}
  \caption{Small-step operational semantics (retained rules)}
  \label{fig:operational-semantics}
\end{figure}


\begin{figure}
  \centering
  \begin{framed}
  
\infrule[E-PatNat-Red]
{ t \longrightarrow^0 t'}
{ \tpd{(\patmat{t}{\patnat{n}}{t_2}{t_3})}{T} \longrightarrow^0 \tpd{(\patmat{t'}{\patnat{n}}{t_2}{t_3})}{T} }

\infax[E-PatNat-Succ]
{ \tpd{(\patmat{\tpd{(\quoted{n})}{\qtype{Nat}}}{\patnat{n}}{t_2}{t_3})}{T} \longrightarrow^0 t_2 }

\infrule[E-PatNat-Fail]
{ \hat\t \not= n }
{ \tpd{(\patmat{\tpd{(\quoted{\hat\t})}{\qtype{Nat}}}{\patnat{n}}{t_2}{t_3})}{T} \longrightarrow^0 t_3 }

\vspace{1mm}

\infrule[E-PatVar-Red]
{ t \longrightarrow^0 t' }
{ \tpd{(\patmat{t}{\patref{x}}{t_2}{t_3})}{T} \longrightarrow^0 \tpd{(\patmat{t'}{\patref{x}}{t_2}{t_3})}{T} }

\infax[E-PatVar-Succ]
{ \tpd{(\patmat{\tpd{(\quoted{x})}{\qtype{T_1}}}{\patref{x}}{t_2}{t_3})}{T} \longrightarrow^0 t_2 }

\infrule[E-PatVar-Fail]
{ \hat\t \not= x }
{ \tpd{(\patmat{\tpd{(\quoted{\hat\t})}{\qtype{T_1}}}{\patref{x}}{t_2}{t_3})}{T} \longrightarrow^0 t_3 }

\vspace{1mm}

\infrule[E-PatApp-Red]
{ t \longrightarrow^0 t'}
{ \tpd{(\patmat{t}{\patapp{(\patbind{b_1}{T_1 \to T_2})}{(\patbind{b_0}{T_1})}}{t_2}{t_3})}{T} \longrightarrow^0 \tpd{(\patmat{t'}{\patapp{(\patbind{b_1}{T_1 \to T_2})}{(\patbind{b_0}{T_1})}}{t_2}{t_3})}{T} }

\infrule[E-PatApp-Succ]
{ \hat\t = (e_1 \; e_2) }
{ \tpd{(\patmat{\tpd{(\quoted{\hat\t})}{\qtype{T_3}}}{\patapp{(\patbind{b_1}{T_1 \to T_2})}{(\patbind{b_0}{T_1})}}{t_2}{t_3})}{T} \longrightarrow^0 t_2[b_0 \mapsto \quoted{e_2}][b_1 \mapsto \quoted{e_1}] }

\infrule[E-PatApp-Fail]
{ \hat\t \not= (e_1 \; e_2) }
{ \tpd{(\patmat{\tpd{(\quoted{\hat\t})}{\qtype{T_3}}}{\patapp{(\patbind{b_1}{T_1 \to T_2})}{(\patbind{b_0}{T_1})}}{t_2}{t_3})}{T} \longrightarrow^0 t_3 }


\vspace{1mm}

\infrule[E-PatUnlift-Red]
{ t \longrightarrow^0 t'}
{ \tpd{(\patmat{t}{\patunlift{b_0}}{t_2}{t_3})}{T} \longrightarrow^0 \tpd{(\patmat{t'}{\patunlift{b_0}}{t_2}{t_3})}{T} }

\infax[E-PatUnlift-Succ]
{ \tpd{(\patmat{\tpd{(\quoted{n})}{\qtype{Nat}}}{\patunlift{b_0}}{t_2}{t_3})}{T} \longrightarrow^0 t_2[b_0 \mapsto n] }

\infrule[E-PatUnlift-Fail]
{ \hat\t \not= n }
{ \tpd{(\patmat{\tpd{(\quoted{\hat\t})}{\qtype{T_1}}}{\patunlift{b_0}}{t_2}{t_3})}{T} \longrightarrow^0 t_3 }

\vspace{1mm}

\infrule[E-PatLam-Red]
{ t \longrightarrow^0 t'}
{ \tpd{(\patmat{t}{\patlam{b_0}{T_1 \to T_2}}{t_2}{t_3})}{T} \longrightarrow^0 \tpd{(\patmat{t'}{\patlam{b_0}{T_1 \to T_2}}{t_2}{t_3})}{T} }

\infrule[E-PatLam-Succ]
{ \hat\t = \lam{x}{T_1}{e} \andalso x' \text{ is fresh} }
{ \tpd{(\patmat{\tpd{(\quoted{\hat\t})}{\qtype{(T_1 \to T_2)}}}{\patlam{b_0}{T_1 \to T_2}}{t_2}{t_3})}{T} \longrightarrow^0 t_2[b_0 \mapsto \lam{x'}{\qtype{T_1}}{\quoted{e[x \mapsto \splice{x'}]}}] }

\infrule[E-PatLam-Fail]
{ \hat\t \not= \lam{x}{T_1}{e} }
{ \tpd{(\patmat{\tpd{(\quoted{\hat\t})}{\qtype{T_1 \to T_2}}}{\patlam{b_0}{T_1 \to T_2}}{t_2}{t_3})}{T} \longrightarrow^0 t_3 }

\vspace{1mm}

\infrule[E-PatFix-Red]
{ t \longrightarrow^0 t'}
{ \tpd{(\patmat{t}{\patfix{(\patbind{b_0}{T_1 \to T_1})}}{t_2}{t_3})}{T} \longrightarrow^0 \tpd{(\patmat{t'}{\patfix{(\patbind{b_0}{T_1 \to T_1})}}{t_2}{t_3})}{T} }

\infrule[E-PatFix-Succ]
{ \hat\t = \fix{e} }
{ \tpd{(\patmat{(\tpd{\quoted{\hat\t}}{\qtype{T_1}})}{\patfix{(\patbind{b_0}{T_1 \to T_1})}}{t_2}{t_3})}{T} \longrightarrow^0 t_2[b_0 \mapsto \quoted{e}] }

\infrule[E-PatFix-Fail]
{ \hat\t \not= \fix{t'} }
{ \tpd{(\patmat{(\tpd{\quoted{\hat\t}}{\qtype{T_1}})}{\patfix{(\patbind{b_0}{T_1 \to T_1})}}{t_2}{t_3})}{T} \longrightarrow^0 t_3 }

  
  \end{framed}
  \caption{Small-step operational semantics (added rules)}
  \label{fig:pattern-semantics}
\end{figure}


\end{document}